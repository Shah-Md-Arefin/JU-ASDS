{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3ed19-6d8d-498f-9d08-4dcc0690ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== STEP 1: DATA LOADING AND EXPLORATION ===\")\n",
    "\n",
    "# Read the data file\n",
    "df = pd.read_csv(r'E:\\Final_Data_File.csv')\n",
    "print(\"✓ Data loaded successfully from E:\\Final_Data_File.csv\")\n",
    "print(f\"✓ Dataset shape: {df.shape}\")\n",
    "\n",
    "# Basic exploration\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nMissing values summary:\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44ee08-54d4-4391-b52f-e0a84d1c0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VARIABLE MAPPING AND DATA CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n Variable Mapping and Data Cleaning...\")\n",
    "\n",
    "# Updated variable mapping based on your provided list\n",
    "variable_map = {\n",
    "    # Child Health Outcomes (Dependent Variables)\n",
    "    'stunting': 'HW70',        # Height/Age standard deviation\n",
    "    'wasting': 'HW72',         # Weight/Height standard deviation  \n",
    "    'underweight': 'HW71',     # Weight/Age standard deviation\n",
    "    'diarrhea': 'H11',         # Had diarrhea recently\n",
    "    'fever': 'H22',            # Had fever in last two weeks\n",
    "    'ari_cough': 'H31',        # Had cough in last two weeks\n",
    "    'ari_breathing': 'H31B',   # Short, rapid breaths\n",
    "    'ari_nose': 'H31C',        # Problem in chest or blocked/running nose\n",
    "    \n",
    "    # Maternal Mental Health (Independent Variables)\n",
    "    'phq_score': 'MTH22',      # PHQ score (Primary exposure)\n",
    "    'gad_score': 'MTH24',      # GAD score (Primary exposure)\n",
    "    'depression_diagnosed': 'MTH19',  # Ever told have depression\n",
    "    'anxiety_diagnosed': 'MTH20',     # Ever told have anxiety\n",
    "    'mental_health_meds': 'MTH21',    # Took medication for depression/anxiety\n",
    "    'phq_referral': 'MTH23',   # Referral based on PHQ score\n",
    "    \n",
    "    # Women's Empowerment (Moderating Variables)\n",
    "    'education': 'V106',       # Highest educational level\n",
    "    'wealth_index': 'V190',    # Wealth index combined\n",
    "    'residence': 'V025',       # Type of place of residence\n",
    "    'working_status': 'V714',  # Respondent currently working\n",
    "    'occupation': 'V717',      # Respondent's occupation (grouped)\n",
    "    'mobile_phone': 'V169A',   # Owns a mobile telephone\n",
    "    'bank_account': 'V170',    # Has account in bank/financial institution\n",
    "    \n",
    "    # Healthcare Seeking (Mediating Variables)\n",
    "    'anc_visits': 'M14',       # Number of antenatal visits\n",
    "    'delivery_place': 'M15',   # Place of delivery\n",
    "    'c_section': 'M17',        # Delivery by caesarean section\n",
    "    'exclusive_bf': 'S435BH',  # Exclusive breastfeeding\n",
    "    \n",
    "    # Control Variables\n",
    "    'mother_age': 'V012',      # Respondent's current age\n",
    "    'child_age_months': 'HW1', # Child's age in months\n",
    "    'child_sex': 'B4',         # Sex of child\n",
    "    'birth_order': 'BORD',     # Birth order number\n",
    "    'region': 'V024',          # Division\n",
    "    'marital_status': 'V501',  # Current marital status\n",
    "    'bmi': 'V445',             # Body Mass Index\n",
    "    'children_ever_born': 'V201',  # Total children ever born\n",
    "    'sample_weight': 'V005'    # Sample weight\n",
    "}\n",
    "\n",
    "# Display available variables\n",
    "print(\"Key Variables Available:\")\n",
    "available_vars = []\n",
    "for key, value in variable_map.items():\n",
    "    if value in df.columns:\n",
    "        available_vars.append(key)\n",
    "        print(f\"✓ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nTotal key variables available: {len(available_vars)}/{len(variable_map)}\")\n",
    "\n",
    "# Create a working copy of the dataset\n",
    "analysis_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af025d4a-146c-4f55-a6c7-8f762e41551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CLEANING AND VARIABLE CREATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nData Cleaning and Variable Creation...\")\n",
    "# Fix Z-score scaling and filter data\n",
    "analysis_df['HW70'] = pd.to_numeric(analysis_df['HW70'], errors='coerce') / 100  # Stunting\n",
    "analysis_df['HW71'] = pd.to_numeric(analysis_df['HW71'], errors='coerce') / 100  # Underweight\n",
    "analysis_df['HW72'] = pd.to_numeric(analysis_df['HW72'], errors='coerce') / 100  # Wasting\n",
    "\n",
    "# Fix weight variable scaling\n",
    "analysis_df['V005'] = pd.to_numeric(analysis_df['V005'], errors='coerce') / 1000000\n",
    "\n",
    "# Filter for children under 5 years and most recent births\n",
    "analysis_df['HW1'] = pd.to_numeric(analysis_df['HW1'], errors='coerce')\n",
    "analysis_df = analysis_df[analysis_df['HW1'] <= 60]  # Under 5 years\n",
    "#analysis_df = analysis_df[analysis_df['BIDX'] == 1]  # Most recent births\n",
    "print(f\"Final sample size: {len(analysis_df)} mother-child pairs\")\n",
    "\n",
    "# Create Child Health Outcome Variables\n",
    "def create_child_health_outcomes(df):\n",
    "    \"\"\"Create comprehensive child health outcome variables\"\"\"\n",
    "    print(\"Creating child health outcome variables...\")\n",
    "    \n",
    "    # Nutritional outcomes (WHO standards)\n",
    "    df['stunting_binary'] = (df['HW70'] < -2).astype(int)\n",
    "    df['wasting_binary'] = (df['HW72'] < -2).astype(int)  \n",
    "    df['underweight_binary'] = (df['HW71'] < -2).astype(int)\n",
    "    \n",
    "    # Severe forms\n",
    "    df['severe_stunting'] = (df['HW70'] < -3).astype(int)\n",
    "    df['severe_wasting'] = (df['HW72'] < -3).astype(int)\n",
    "    df['severe_underweight'] = (df['HW71'] < -3).astype(int)\n",
    "    \n",
    "    # Composite malnutrition indicator\n",
    "    df['any_malnutrition'] = ((df['stunting_binary'] == 1) | \n",
    "                             (df['wasting_binary'] == 1) | \n",
    "                             (df['underweight_binary'] == 1)).astype(int)\n",
    "    \n",
    "    # Morbidity outcomes\n",
    "    df['diarrhea_binary'] = (df['H11'] == 'Yes, last two weeks').astype(int)\n",
    "    df['fever_binary'] = (df['H22'] == 'Yes').astype(int)\n",
    "    df['cough_binary'] = (df['H31'] == 'Yes, last two weeks').astype(int)\n",
    "    \n",
    "    # Create ARI variable (cough + breathing problems)\n",
    "    if 'H31B' in df.columns:\n",
    "        df['ari_binary'] = ((df['H31'] == 'Yes, last two weeks') & \n",
    "                           (df['H31B'] == 'Yes')).astype(int)\n",
    "    \n",
    "    # Composite morbidity indicator\n",
    "    df['any_morbidity'] = ((df['diarrhea_binary'] == 1) | \n",
    "                          (df['fever_binary'] == 1) | \n",
    "                          (df['cough_binary'] == 1)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "analysis_df = create_child_health_outcomes(analysis_df)\n",
    "\n",
    "# Create Maternal Mental Health Variables\n",
    "def create_mental_health_variables(df):\n",
    "    \"\"\"Create comprehensive maternal mental health indicators\"\"\"\n",
    "    print(\"Creating maternal mental health variables...\")\n",
    "    \n",
    "    # Convert PHQ-9 score to numeric\n",
    "    df['phq9_score'] = pd.to_numeric(df['MTH22'], errors='coerce')\n",
    "    \n",
    "    # Create depression categories based on PHQ-9\n",
    "    conditions_depression = [\n",
    "        df['phq9_score'] < 5,\n",
    "        (df['phq9_score'] >= 5) & (df['phq9_score'] < 10),\n",
    "        (df['phq9_score'] >= 10) & (df['phq9_score'] < 15),\n",
    "        (df['phq9_score'] >= 15) & (df['phq9_score'] < 20),\n",
    "        df['phq9_score'] >= 20\n",
    "    ]\n",
    "    choices_depression = ['None', 'Mild', 'Moderate', 'Moderately Severe', 'Severe']\n",
    "    df['depression_category'] = np.select(conditions_depression, choices_depression, default='Unknown')\n",
    "    \n",
    "    # Binary depression (mild to severe)\n",
    "    df['depression_binary'] = (df['phq9_score'] >= 5).astype(int)\n",
    "    \n",
    "    # Convert GAD-7 score to numeric\n",
    "    df['gad7_score'] = pd.to_numeric(df['MTH24'], errors='coerce')\n",
    "    \n",
    "    # Create anxiety categories based on GAD-7\n",
    "    conditions_anxiety = [\n",
    "        df['gad7_score'] < 6,\n",
    "        (df['gad7_score'] >= 6) & (df['gad7_score'] < 15),\n",
    "        df['gad7_score'] >= 15\n",
    "    ]\n",
    "    choices_anxiety = ['Mild', 'Moderate', 'Severe']\n",
    "    df['anxiety_category'] = np.select(conditions_anxiety, choices_anxiety, default='Unknown')\n",
    "    \n",
    "    # Binary anxiety (mild to severe)\n",
    "    df['anxiety_binary'] = (df['gad7_score'] >= 4).astype(int)\n",
    "    \n",
    "    # Composite mental health burden\n",
    "    df['any_mental_health_issue'] = ((df['depression_binary'] == 1) | \n",
    "                                    (df['anxiety_binary'] == 1)).astype(int)\n",
    "    \n",
    "    # Mental health severity score (combined)\n",
    "    df['mental_health_severity'] = (df['phq9_score'].fillna(0) + df['gad7_score'].fillna(0)) / 2\n",
    "    \n",
    "    # Diagnosis and treatment variables\n",
    "    df['ever_depression_diagnosed'] = (df['MTH19'] == 'Yes').astype(int)\n",
    "    df['ever_anxiety_diagnosed'] = (df['MTH20'] == 'Yes').astype(int)\n",
    "    df['mental_health_medication'] = (df['MTH21'] == 'Yes').astype(int)\n",
    "    df['phq_referral'] = (df['MTH23'] == 'Yes').astype(int)\n",
    "    \n",
    "    print(f\"PHQ-9 scores available: {df['phq9_score'].notna().sum()}\")\n",
    "    print(f\"GAD-7 scores available: {df['gad7_score'].notna().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "analysis_df = create_mental_health_variables(analysis_df)\n",
    "\n",
    "# Create Women's Empowerment Variables\n",
    "def create_empowerment_variables(df):\n",
    "    \"\"\"Create comprehensive women's empowerment indicators\"\"\"\n",
    "    print(\"Creating women's empowerment variables...\")\n",
    "    \n",
    "    # Education level\n",
    "    education_mapping = {\n",
    "        'No education': 'No education',\n",
    "        'Primary': 'Primary',\n",
    "        'Secondary': 'Secondary', \n",
    "        'Higher': 'Higher'\n",
    "    }\n",
    "    df['education_level'] = df['V106'].map(education_mapping)\n",
    "    \n",
    "    # Wealth quintile (ensure it's categorical)\n",
    "    df['wealth_quintile'] = df['V190'].astype(str)\n",
    "    \n",
    "    # Residence\n",
    "    df['urban_rural'] = df['V025']\n",
    "    \n",
    "    # Employment and economic empowerment\n",
    "    df['currently_working'] = (df['V714'] == 'Yes').astype(int)\n",
    "    df['has_mobile'] = (df['V169A'] == 'Yes').astype(int)\n",
    "    df['has_bank_account'] = (df['V170'] == 'Yes').astype(int)\n",
    "    \n",
    "    # Create empowerment index (composite)\n",
    "    empowerment_components = ['currently_working', 'has_mobile', 'has_bank_account']\n",
    "    df['empowerment_index'] = df[empowerment_components].sum(axis=1) / len(empowerment_components)\n",
    "    df['empowerment_category'] = pd.cut(df['empowerment_index'], \n",
    "                                       bins=[0, 0.33, 0.66, 1], \n",
    "                                       labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "analysis_df = create_empowerment_variables(analysis_df)\n",
    "\n",
    "# Create Healthcare Seeking Variables\n",
    "def create_healthcare_variables(df):\n",
    "    \"\"\"Create healthcare seeking behavior indicators\"\"\"\n",
    "    print(\"Creating healthcare seeking variables...\")\n",
    "    \n",
    "    # ANC visits (adequate = 4+ visits)\n",
    "    df['anc_visits'] = pd.to_numeric(df['M14'], errors='coerce')\n",
    "    df['adequate_anc'] = (df['anc_visits'] >= 4).astype(int)\n",
    "    \n",
    "    # Skilled birth attendance\n",
    "    skilled_facilities = ['Private clinic', 'UH & family welfare center', 'District hospital', 'Private hospital','Her home','Medical college hospital','NGO static clinic','Community clinic','Upazila health complex','Other public sector','MCWC','Specialized govt hospital','Private medical college hospital','Other private medical','Other','Delivery Hut']\n",
    "    df['skilled_birth'] = df['M15'].isin(skilled_facilities).astype(int)\n",
    "\n",
    "    # C-section\n",
    "    df['c_section_binary'] = (df['M17'] == 'Yes').astype(int)\n",
    "    \n",
    "    # Breastfeeding practices\n",
    "    df['exclusive_bf_binary'] = (df['S435BH'] == 'Yes').astype(int)\n",
    "    \n",
    "    # Composite healthcare quality index\n",
    "    healthcare_components = ['adequate_anc', 'skilled_birth']\n",
    "    df['healthcare_quality_index'] = df[healthcare_components].sum(axis=1) / len(healthcare_components)\n",
    "    \n",
    "    return df\n",
    "\n",
    "analysis_df = create_healthcare_variables(analysis_df)\n",
    "\n",
    "# Create Control Variables\n",
    "def create_control_variables(df):\n",
    "    \"\"\"Create control variables\"\"\"\n",
    "    print(\"Creating control variables...\")\n",
    "    \n",
    "    # Mother's characteristics\n",
    "    df['mother_age'] = pd.to_numeric(df['V012'], errors='coerce')\n",
    "    df['mother_age_group'] = pd.cut(df['mother_age'], \n",
    "                                   bins=[15, 25, 35, 49], \n",
    "                                   labels=['15-24', '25-34', '35-49'])\n",
    "    df['bmi'] = pd.to_numeric(df['V445'], errors='coerce')\n",
    "    df['parity'] = pd.to_numeric(df['V201'], errors='coerce')\n",
    "    df['married'] = (df['V501'] == 'Married').astype(int)\n",
    "    \n",
    "    # Child's characteristics\n",
    "    df['child_age_months'] = pd.to_numeric(df['HW1'], errors='coerce')\n",
    "    df['child_age_group'] = pd.cut(df['child_age_months'],\n",
    "                                  bins=[0, 12, 24, 36, 48, 60],\n",
    "                                  labels=['0-11m', '12-23m', '24-35m', '36-47m', '48-59m'])\n",
    "    df['child_sex'] = df['B4']\n",
    "    df['birth_order'] = pd.to_numeric(df['BORD'], errors='coerce')\n",
    "    \n",
    "    # Household characteristics\n",
    "    df['region'] = df['V024']\n",
    "    df['sample_weight'] = pd.to_numeric(df['V005'], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "analysis_df = create_control_variables(analysis_df)\n",
    "\n",
    "print(f\"\\nData preparation completed!\")\n",
    "print(f\"Final dataset: {len(analysis_df)} observations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ae067-92ab-4879-be73-c2e376367432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAMPLE CHARACTERISTICS - TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== STEP 4: SAMPLE CHARACTERISTICS ===\")\n",
    "\n",
    "print(f\"Total original sample size: {len(df):,}\")\n",
    "print(f\"Final analysis sample size: {len(analysis_df):,}\")\n",
    "\n",
    "def generate_descriptive_statistics(df):\n",
    "    \"\"\"Generate comprehensive descriptive statistics\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DESCRIPTIVE STATISTICS - PERINATAL MENTAL HEALTH CASCADE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Maternal Mental Health Prevalence\n",
    "    print(\"\\n MATERNAL MENTAL HEALTH PREVALENCE:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if 'phq9_score' in df.columns:\n",
    "        total_with_phq = df['phq9_score'].notna().sum()\n",
    "        depression_prev = df['depression_binary'].mean() * 100\n",
    "        mean_phq = df['phq9_score'].mean()\n",
    "        \n",
    "        print(f\"PHQ-9 (Depression):\")\n",
    "        print(f\"  • Sample size: {total_with_phq}\")\n",
    "        print(f\"  • Mild-to-Severe (≥5): {depression_prev:.1f}%\")\n",
    "        print(f\"  • Mean score: {mean_phq:.2f}\")\n",
    "        \n",
    "        # Depression severity distribution\n",
    "        depression_dist = df['depression_category'].value_counts()\n",
    "        print(f\"  • Severity distribution:\")\n",
    "        for category, count in depression_dist.items():\n",
    "            percentage = (count / total_with_phq) * 100\n",
    "            print(f\"    {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    if 'gad7_score' in df.columns:\n",
    "        total_with_gad = df['gad7_score'].notna().sum()\n",
    "        anxiety_prev = df['anxiety_binary'].mean() * 100\n",
    "        mean_gad = df['gad7_score'].mean()\n",
    "        \n",
    "        print(f\"\\nGAD-7 (Anxiety):\")\n",
    "        print(f\"  • Sample size: {total_with_gad}\")\n",
    "        print(f\"  • Mild-to-Severe (≥4): {anxiety_prev:.1f}%\")\n",
    "        print(f\"  • Mean score: {mean_gad:.2f}\")\n",
    "        \n",
    "        # Anxiety severity distribution\n",
    "        anxiety_dist = df['anxiety_category'].value_counts()\n",
    "        print(f\"  • Severity distribution:\")\n",
    "        for category, count in anxiety_dist.items():\n",
    "            percentage = (count / total_with_gad) * 100\n",
    "            print(f\"    {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Comorbidity\n",
    "    if all(var in df.columns for var in ['depression_binary', 'anxiety_binary']):\n",
    "        comorbidity = ((df['depression_binary'] == 1) & (df['anxiety_binary'] == 1)).mean() * 100\n",
    "        any_mental_health = df['any_mental_health_issue'].mean() * 100\n",
    "        print(f\"\\nMental Health Comorbidity:\")\n",
    "        print(f\"  • Any mental health issue: {any_mental_health:.1f}%\")\n",
    "        print(f\"  • Both depression & anxiety: {comorbidity:.1f}%\")\n",
    "    \n",
    "    # Child Health Outcomes\n",
    "    print(\"\\n CHILD HEALTH OUTCOMES:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    nutrition_indicators = {\n",
    "        'Stunting': 'stunting_binary',\n",
    "        'Severe Stunting': 'severe_stunting',\n",
    "        'Wasting': 'wasting_binary', \n",
    "        'Severe Wasting': 'severe_wasting', \n",
    "        'Underweight': 'underweight_binary',\n",
    "        'Severe Underweight': 'severe_underweight',\n",
    "        'Any Malnutrition': 'any_malnutrition'\n",
    "    }\n",
    "    \n",
    "    for name, var in nutrition_indicators.items():\n",
    "        if var in df.columns:\n",
    "            prev = df[var].mean() * 100\n",
    "            count = df[var].sum()\n",
    "            total = df[var].notna().sum()\n",
    "            print(f\"  • {name}: {prev:.1f}% ({count}/{total})\")\n",
    "    \n",
    "    morbidity_indicators = {\n",
    "        'Diarrhea': 'diarrhea_binary',\n",
    "        'Fever': 'fever_binary',\n",
    "        'Cough': 'cough_binary',\n",
    "        'ARI': 'ari_binary',\n",
    "        'Any Morbidity': 'any_morbidity'\n",
    "    }\n",
    "    \n",
    "    for name, var in morbidity_indicators.items():\n",
    "        if var in df.columns:\n",
    "            prev = df[var].mean() * 100\n",
    "            count = df[var].sum()\n",
    "            total = df[var].notna().sum()\n",
    "            print(f\"  • {name}: {prev:.1f}% ({count}/{total})\")\n",
    "    \n",
    "    # Socio-demographic Characteristics\n",
    "    print(\"\\n SOCIO-DEMOGRAPHIC CHARACTERISTICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if 'mother_age' in df.columns:\n",
    "        print(f\"  • Mother's age: {df['mother_age'].mean():.1f} years (SD: {df['mother_age'].std():.1f})\")\n",
    "    \n",
    "    if 'child_age_months' in df.columns:\n",
    "        print(f\"  • Child's age: {df['child_age_months'].mean():.1f} months (SD: {df['child_age_months'].std():.1f})\")\n",
    "    \n",
    "    if 'education_level' in df.columns:\n",
    "        education_dist = df['education_level'].value_counts(normalize=True) * 100\n",
    "        print(f\"  • Education:\")\n",
    "        for level, percent in education_dist.items():\n",
    "            print(f\"    {level}: {percent:.1f}%\")\n",
    "    \n",
    "    if 'wealth_quintile' in df.columns:\n",
    "        wealth_dist = df['wealth_quintile'].value_counts(normalize=True) * 100\n",
    "        print(f\"  • Wealth quintile:\")\n",
    "        for quintile, percent in wealth_dist.items():\n",
    "            print(f\"    {quintile}: {percent:.1f}%\")\n",
    "    \n",
    "    if 'urban_rural' in df.columns:\n",
    "        residence_dist = df['urban_rural'].value_counts(normalize=True) * 100\n",
    "        print(f\"  • Residence:\")\n",
    "        for residence, percent in residence_dist.items():\n",
    "            print(f\"    {residence}: {percent:.1f}%\")\n",
    "\n",
    "generate_descriptive_statistics(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742f558-d1f7-4569-94f1-304f1116881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS \n",
    "# =============================================================================\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== PLOT 1.1: STUNTING BY MATERNAL DEPRESSION ===\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate the data\n",
    "contingency_table = pd.crosstab(analysis_df['depression_binary'], analysis_df['stunting_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['depression_binary'], analysis_df['stunting_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars\n",
    "categories = ['No Depression', 'Depression']\n",
    "normal_height = [cross_tab.iloc[0, 0], cross_tab.iloc[1, 0]]  # Normal height percentages\n",
    "stunted = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]        # Stunted percentages\n",
    "\n",
    "# Counts for labels\n",
    "normal_height_counts = [contingency_table.iloc[0, 0], contingency_table.iloc[1, 0]]\n",
    "stunted_counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "total_counts = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart\n",
    "bars_normal = plt.bar(x_pos - bar_width/2, normal_height, bar_width, \n",
    "                      color='lightblue', label='Normal Height', \n",
    "                      edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_stunted = plt.bar(x_pos + bar_width/2, stunted, bar_width, \n",
    "                       color='coral', label='Stunted',\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_normal, normal_height, normal_height_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_stunted, stunted, stunted_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Child Nutritional Status by Maternal Depression', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Maternal Depression Status', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(normal_height + stunted) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: NUTRITIONAL STATUS BY MATERNAL DEPRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, group in enumerate(['No Depression', 'Depression']):\n",
    "    summary_data.append({\n",
    "        'Group': group,\n",
    "        'Total N': total_counts[i],\n",
    "        'Normal Height': f\"{normal_height[i]:.1f}% (n={normal_height_counts[i]})\",\n",
    "        'Stunted': f\"{stunted[i]:.1f}% (n={stunted_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    diff = abs(stunted[0] - stunted[1])\n",
    "    direction = \"higher\" if stunted[1] > stunted[0] else \"lower\"\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Stunting prevalence is {diff:.1f} percentage points {direction} in children of depressed mothers\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Similar stunting patterns observed across maternal depression status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3434c-d3e8-4e49-88ef-c0d0671f5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Side-by-side comparison of stunting prevalence only\n",
    "print(\"=== SIDE-BY-SIDE STUNTING BY MATERNAL DEPRESSION ===\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Extract just stunting prevalence\n",
    "stunting_prev = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]\n",
    "groups = ['No Depression', 'Depression']\n",
    "counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "totals = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "bars = plt.bar(groups, stunting_prev, color=['#2E86AB', '#A23B72'], alpha=0.8, \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, (prev, count, total) in enumerate(zip(stunting_prev, counts, totals)):\n",
    "    plt.text(i, prev + 1, f'{prev:.1f}%\\n({count}/{total})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Child Stunting Prevalence by Maternal Depression Status', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Stunting Prevalence (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylim(0, max(stunting_prev) + 5)\n",
    "\n",
    "# Clean styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fd1f7-1e99-4f71-852d-d54c5e5a8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANXIETY VERSION - CLUSTERED BAR CHART\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== STUNTING BY MATERNAL ANXIETY ===\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate the data for ANXIETY\n",
    "contingency_table = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['stunting_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['stunting_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars\n",
    "categories = ['No Anxiety', 'Anxiety']\n",
    "normal_height = [cross_tab.iloc[0, 0], cross_tab.iloc[1, 0]]  # Normal height percentages\n",
    "stunted = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]        # Stunted percentages\n",
    "\n",
    "# Counts for labels\n",
    "normal_height_counts = [contingency_table.iloc[0, 0], contingency_table.iloc[1, 0]]\n",
    "stunted_counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "total_counts = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# For anxiety charts - Professional Blue & Orange\n",
    "bars_normal = plt.bar(x_pos - bar_width/2, normal_height, bar_width, \n",
    "                      color='#87CEEB', label='Normal Height',  # Sky Blue\n",
    "                      edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_stunted = plt.bar(x_pos + bar_width/2, stunted, bar_width, \n",
    "                       color='#FFA500', label='Stunted',        # Orange\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_normal, normal_height, normal_height_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_stunted, stunted, stunted_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Child Nutritional Status by Maternal Anxiety', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Maternal Anxiety Status', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(normal_height + stunted) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis for anxiety\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: NUTRITIONAL STATUS BY MATERNAL ANXIETY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, group in enumerate(['No Anxiety', 'Anxiety']):\n",
    "    summary_data.append({\n",
    "        'Group': group,\n",
    "        'Total N': total_counts[i],\n",
    "        'Normal Height': f\"{normal_height[i]:.1f}% (n={normal_height_counts[i]})\",\n",
    "        'Stunted': f\"{stunted[i]:.1f}% (n={stunted_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b2dea-4914-4643-a930-d31a99323f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIDE-BY-SIDE COMPARISON: STUNTING BY MATERNAL ANXIETY\n",
    "# =============================================================================\n",
    "print(\"=== PLOT 1.4: SIDE-BY-SIDE STUNTING BY MATERNAL DEPRESSION ===\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Calculate data for ANXIETY\n",
    "contingency_table = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['stunting_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['stunting_binary'], normalize='index') * 100\n",
    "\n",
    "# Extract just stunting prevalence\n",
    "stunting_prev = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]\n",
    "groups = ['No Anxiety', 'Anxiety']\n",
    "counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "totals = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Anxiety colors: Green & Purple theme\n",
    "bars = plt.bar(groups, stunting_prev, color=['#20B2AA', '#DAA520'], alpha=0.8, \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, (prev, count, total) in enumerate(zip(stunting_prev, counts, totals)):\n",
    "    plt.text(i, prev + 1, f'{prev:.1f}%\\n({count}/{total})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Child Stunting Prevalence by Maternal Anxiety Status', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Stunting Prevalence (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylim(0, max(stunting_prev) + 5)\n",
    "\n",
    "# Clean styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526fc86-6aee-48be-a1a0-0aeeba48889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS - PLOT\n",
    "# =============================================================================\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== PLOT 2.1: WASTING BY MATERNAL DEPRESSION ===\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate the data\n",
    "contingency_table = pd.crosstab(analysis_df['depression_binary'], analysis_df['wasting_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['depression_binary'], analysis_df['wasting_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars\n",
    "categories = ['No Depression', 'Depression']\n",
    "not_wasted = [cross_tab.iloc[0, 0], cross_tab.iloc[1, 0]]  # Not wasted percentages\n",
    "wasted = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]      # Wasted percentages\n",
    "\n",
    "# Counts for labels\n",
    "not_wasted_counts = [contingency_table.iloc[0, 0], contingency_table.iloc[1, 0]]\n",
    "wasted_counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "total_counts = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart\n",
    "bars_normal = plt.bar(x_pos - bar_width/2, not_wasted, bar_width, \n",
    "                      color='lightblue', label='Not Wasted', \n",
    "                      edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_wasted = plt.bar(x_pos + bar_width/2, wasted, bar_width, \n",
    "                       color='coral', label='Wasted',\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_normal, not_wasted, not_wasted_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_wasted, wasted, wasted_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Child Wasting Status by Maternal Depression', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Maternal Depression Status', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(not_wasted + wasted) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: WASTING STATUS BY MATERNAL DEPRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, group in enumerate(['No Depression', 'Depression']):\n",
    "    summary_data.append({\n",
    "        'Group': group,\n",
    "        'Total N': total_counts[i],\n",
    "        'Not Wasted': f\"{not_wasted[i]:.1f}% (n={not_wasted_counts[i]})\",\n",
    "        'Wasted': f\"{wasted[i]:.1f}% (n={wasted_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    diff = abs(wasted[0] - wasted[1])\n",
    "    direction = \"higher\" if wasted[1] > wasted[0] else \"lower\"\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Wasting prevalence is {diff:.1f} percentage points {direction} in children of depressed mothers\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Similar wasting patterns observed across maternal depression status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bdb15-1d15-43e2-9103-0ca6cd050a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Side-by-side comparison of wasting prevalence only\n",
    "print(\"=== SIDE-BY-SIDE WASTING BY MATERNAL DEPRESSION ===\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Extract just wasting prevalence\n",
    "wasting_prev = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]\n",
    "groups = ['No Depression', 'Depression']\n",
    "counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "totals = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "bars = plt.bar(groups, wasting_prev, color=['#2E86AB', '#A23B72'], alpha=0.8, \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, (prev, count, total) in enumerate(zip(wasting_prev, counts, totals)):\n",
    "    plt.text(i, prev + 1, f'{prev:.1f}%\\n({count}/{total})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Child Wasting Prevalence by Maternal Depression Status', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Wasting Prevalence (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylim(0, max(wasting_prev) + 5)\n",
    "\n",
    "# Clean styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894ffbd-c57e-406e-9b3c-67510a87e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANXIETY VERSION - CLUSTERED BAR CHART FOR WASTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== WASTING BY MATERNAL ANXIETY ===\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate the data for ANXIETY and WASTING\n",
    "contingency_table = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['wasting_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['wasting_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars\n",
    "categories = ['No Anxiety', 'Anxiety']\n",
    "not_wasted = [cross_tab.iloc[0, 0], cross_tab.iloc[1, 0]]  # Not wasted percentages\n",
    "wasted = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]      # Wasted percentages\n",
    "\n",
    "# Counts for labels\n",
    "not_wasted_counts = [contingency_table.iloc[0, 0], contingency_table.iloc[1, 0]]\n",
    "wasted_counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "total_counts = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# For anxiety charts - Professional Blue & Orange\n",
    "bars_normal = plt.bar(x_pos - bar_width/2, not_wasted, bar_width, \n",
    "                      color='#87CEEB', label='Not Wasted',  # Sky Blue\n",
    "                      edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_wasted = plt.bar(x_pos + bar_width/2, wasted, bar_width, \n",
    "                       color='#FFA500', label='Wasted',        # Orange\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_normal, not_wasted, not_wasted_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_wasted, wasted, wasted_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Child Wasting Status by Maternal Anxiety', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Maternal Anxiety Status', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(not_wasted + wasted) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis for anxiety\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: WASTING STATUS BY MATERNAL ANXIETY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, group in enumerate(['No Anxiety', 'Anxiety']):\n",
    "    summary_data.append({\n",
    "        'Group': group,\n",
    "        'Total N': total_counts[i],\n",
    "        'Not Wasted': f\"{not_wasted[i]:.1f}% (n={not_wasted_counts[i]})\",\n",
    "        'Wasted': f\"{wasted[i]:.1f}% (n={wasted_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ce183-3ee2-49ea-9a6c-92601b41fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIDE-BY-SIDE COMPARISON: WASTING BY MATERNAL ANXIETY\n",
    "# =============================================================================\n",
    "print(\"=== SIDE-BY-SIDE WASTING BY ANXIETY ===\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Calculate data for ANXIETY\n",
    "contingency_table = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['wasting_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['wasting_binary'], normalize='index') * 100\n",
    "\n",
    "# Extract just stunting prevalence\n",
    "stunting_prev = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]\n",
    "groups = ['No Anxiety', 'Anxiety']\n",
    "counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "totals = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Anxiety colors: Green & Purple theme\n",
    "bars = plt.bar(groups, wasting_prev, color=['#20B2AA', '#DAA520'], alpha=0.8, \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, (prev, count, total) in enumerate(zip(stunting_prev, counts, totals)):\n",
    "    plt.text(i, prev + 1, f'{prev:.1f}%\\n({count}/{total})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Child Wasting Prevalence by Maternal Anxiety Status', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Wasting Prevalence (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylim(0, max(stunting_prev) + 5)\n",
    "\n",
    "# Clean styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08ace5-fa73-49fd-93b3-3fba63408540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS - PLOT\n",
    "# =============================================================================\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== UNDERWEIGHT BY MATERNAL DEPRESSION ===\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate the data for UNDERWEIGHT\n",
    "contingency_table = pd.crosstab(analysis_df['depression_binary'], analysis_df['underweight_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['depression_binary'], analysis_df['underweight_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars\n",
    "categories = ['No Depression', 'Depression']\n",
    "normal_weight = [cross_tab.iloc[0, 0], cross_tab.iloc[1, 0]]  # Normal weight percentages\n",
    "underweight = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]    # Underweight percentages\n",
    "\n",
    "# Counts for labels\n",
    "normal_weight_counts = [contingency_table.iloc[0, 0], contingency_table.iloc[1, 0]]\n",
    "underweight_counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "total_counts = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart\n",
    "bars_normal = plt.bar(x_pos - bar_width/2, normal_weight, bar_width, \n",
    "                      color='lightblue', label='Normal Weight', \n",
    "                      edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_underweight = plt.bar(x_pos + bar_width/2, underweight, bar_width, \n",
    "                       color='coral', label='Underweight',\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_normal, normal_weight, normal_weight_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_underweight, underweight, underweight_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Child Underweight Status by Maternal Depression', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Maternal Depression Status', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(normal_weight + underweight) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: UNDERWEIGHT STATUS BY MATERNAL DEPRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, group in enumerate(['No Depression', 'Depression']):\n",
    "    summary_data.append({\n",
    "        'Group': group,\n",
    "        'Total N': total_counts[i],\n",
    "        'Normal Weight': f\"{normal_weight[i]:.1f}% (n={normal_weight_counts[i]})\",\n",
    "        'Underweight': f\"{underweight[i]:.1f}% (n={underweight_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    diff = abs(underweight[0] - underweight[1])\n",
    "    direction = \"higher\" if underweight[1] > underweight[0] else \"lower\"\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Underweight prevalence is {diff:.1f} percentage points {direction} in children of depressed mothers\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Similar underweight patterns observed across maternal depression status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5dfd74-c968-469d-8f9c-f878a433d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Side-by-side comparison of underweight prevalence only\n",
    "print(\"=== SIDE-BY-SIDE UNDERWEIGHT BY MATERNAL DEPRESSION ===\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Calculate data for underweight\n",
    "contingency_table = pd.crosstab(analysis_df['depression_binary'], analysis_df['underweight_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['depression_binary'], analysis_df['underweight_binary'], normalize='index') * 100\n",
    "\n",
    "# Extract just underweight prevalence\n",
    "underweight_prev = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]\n",
    "groups = ['No Depression', 'Depression']\n",
    "counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "totals = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "bars = plt.bar(groups, underweight_prev, color=['#2E86AB', '#A23B72'], alpha=0.8, \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, (prev, count, total) in enumerate(zip(underweight_prev, counts, totals)):\n",
    "    plt.text(i, prev + 1, f'{prev:.1f}%\\n({count}/{total})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Child Underweight Prevalence by Maternal Depression Status', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Underweight Prevalence (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylim(0, max(underweight_prev) + 5)\n",
    "\n",
    "# Clean styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43465c19-3f1d-4a32-9b5b-bcb791ab5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANXIETY VERSION - CLUSTERED BAR CHART FOR UNDERWEIGHT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== UNDERWEIGHT BY MATERNAL ANXIETY ===\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate the data for ANXIETY and UNDERWEIGHT\n",
    "contingency_table = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['underweight_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['underweight_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars\n",
    "categories = ['No Anxiety', 'Anxiety']\n",
    "normal_weight = [cross_tab.iloc[0, 0], cross_tab.iloc[1, 0]]  # Normal weight percentages\n",
    "underweight = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]    # Underweight percentages\n",
    "\n",
    "# Counts for labels\n",
    "normal_weight_counts = [contingency_table.iloc[0, 0], contingency_table.iloc[1, 0]]\n",
    "underweight_counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "total_counts = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# For anxiety charts - Professional Blue & Orange\n",
    "bars_normal = plt.bar(x_pos - bar_width/2, normal_weight, bar_width, \n",
    "                      color='#87CEEB', label='Normal Weight',  # Sky Blue\n",
    "                      edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_underweight = plt.bar(x_pos + bar_width/2, underweight, bar_width, \n",
    "                       color='#FFA500', label='Underweight',        # Orange\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_normal, normal_weight, normal_weight_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_underweight, underweight, underweight_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Child Underweight Status by Maternal Anxiety', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Maternal Anxiety Status', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(normal_weight + underweight) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis for anxiety\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: UNDERWEIGHT STATUS BY MATERNAL ANXIETY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, group in enumerate(['No Anxiety', 'Anxiety']):\n",
    "    summary_data.append({\n",
    "        'Group': group,\n",
    "        'Total N': total_counts[i],\n",
    "        'Normal Weight': f\"{normal_weight[i]:.1f}% (n={normal_weight_counts[i]})\",\n",
    "        'Underweight': f\"{underweight[i]:.1f}% (n={underweight_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f79ef-8ffb-4e4e-bc20-0a46b1f1f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIDE-BY-SIDE COMPARISON: UNDERWEIGHT BY MATERNAL ANXIETY\n",
    "# =============================================================================\n",
    "print(\"=== SIDE-BY-SIDE UNDERWEIGHT BY ANXIETY ===\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Calculate data for ANXIETY and UNDERWEIGHT\n",
    "contingency_table = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['underweight_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['anxiety_binary'], analysis_df['underweight_binary'], normalize='index') * 100\n",
    "\n",
    "# Extract just underweight prevalence\n",
    "underweight_prev = [cross_tab.iloc[0, 1], cross_tab.iloc[1, 1]]\n",
    "groups = ['No Anxiety', 'Anxiety']\n",
    "counts = [contingency_table.iloc[0, 1], contingency_table.iloc[1, 1]]\n",
    "totals = [contingency_table.iloc[0].sum(), contingency_table.iloc[1].sum()]\n",
    "\n",
    "# Anxiety colors: Teal & Gold theme\n",
    "bars = plt.bar(groups, underweight_prev, color=['#20B2AA', '#DAA520'], alpha=0.8, \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, (prev, count, total) in enumerate(zip(underweight_prev, counts, totals)):\n",
    "    plt.text(i, prev + 1, f'{prev:.1f}%\\n({count}/{total})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.title('Child Underweight Prevalence by Maternal Anxiety Status', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Underweight Prevalence (%)', fontsize=12, fontweight='bold')\n",
    "plt.ylim(0, max(underweight_prev) + 5)\n",
    "\n",
    "# Clean styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.2, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis for anxiety and underweight\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nStatistical Test for Underweight by Anxiety:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30eade-57ed-4710-8a10-b222cfe1a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS - PLOT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== PLOT: DEPRESSION BY WEALTH QUINTILE ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Use analysis_df instead of df\n",
    "contingency_table = pd.crosstab(analysis_df['wealth_quintile'], analysis_df['depression_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['wealth_quintile'], analysis_df['depression_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars\n",
    "categories = sorted(analysis_df['wealth_quintile'].unique())\n",
    "no_depression = [cross_tab.loc[cat, 0] for cat in categories]  # No depression percentages\n",
    "depression = [cross_tab.loc[cat, 1] for cat in categories]     # Depression percentages\n",
    "\n",
    "# Counts for labels\n",
    "no_depression_counts = [contingency_table.loc[cat, 0] for cat in categories]\n",
    "depression_counts = [contingency_table.loc[cat, 1] for cat in categories]\n",
    "total_counts = [contingency_table.loc[cat].sum() for cat in categories]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart\n",
    "bars_no_depression = plt.bar(x_pos - bar_width/2, no_depression, bar_width, \n",
    "                             color='skyblue', label='No Depression', \n",
    "                             edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_depression = plt.bar(x_pos + bar_width/2, depression, bar_width, \n",
    "                          color='salmon', label='Depression',\n",
    "                          edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_no_depression, no_depression, no_depression_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_depression, depression, depression_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Depression Prevalence by Wealth Quintile', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Wealth Quintile', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=8)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(no_depression + depression) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: DEPRESSION BY WEALTH QUINTILE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, quintile in enumerate(categories):\n",
    "    summary_data.append({\n",
    "        'Wealth Quintile': quintile,\n",
    "        'Total N': total_counts[i],\n",
    "        'No Depression': f\"{no_depression[i]:.1f}% (n={no_depression_counts[i]})\",\n",
    "        'Depression': f\"{depression[i]:.1f}% (n={depression_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Depression prevalence varies significantly across wealth quintiles\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Depression prevalence is similar across wealth quintiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795b20d-35c5-4245-aa23-caf24cdc3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOCIOECONOMIC GRADIENT PLOT\n",
    "# =============================================================================\n",
    "\n",
    "print(\" Creating Socioeconomic Gradient Plot...\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "if all(var in analysis_df.columns for var in ['wealth_quintile', 'depression_binary']):\n",
    "    wealth_order = ['Poorest', 'Poorer', 'Middle', 'Richer', 'Richest']\n",
    "    wealth_depression = analysis_df.groupby('wealth_quintile')['depression_binary'].mean() * 100\n",
    "    wealth_depression = wealth_depression.reindex(wealth_order)\n",
    "    \n",
    "    if not wealth_depression.empty:\n",
    "        # Remove any NaN values\n",
    "        valid_indices = [i for i, val in enumerate(wealth_depression.values) if np.isfinite(val)]\n",
    "        valid_values = [wealth_depression.values[i] for i in valid_indices]\n",
    "        valid_labels = [wealth_depression.index[i] for i in valid_indices]\n",
    "        \n",
    "        if valid_values:\n",
    "            plt.plot(range(len(valid_values)), valid_values, \n",
    "                    marker='o', linewidth=3, markersize=12, color='#e74c3c', \n",
    "                    markerfacecolor='white', markeredgewidth=3)\n",
    "            plt.title('Socioeconomic Gradient: Depression by Wealth Quintile\\nBDHS 2022', \n",
    "                     fontweight='bold', fontsize=16)\n",
    "            plt.xlabel('Wealth Quintile', fontsize=12)\n",
    "            plt.ylabel('Depression Prevalence (%)', fontsize=12)\n",
    "            plt.xticks(range(len(valid_values)), valid_labels, rotation=0)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ylim(0, max(valid_values) * 1.15)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, value in enumerate(valid_values):\n",
    "                plt.text(i, value + 1, f'{value:.1f}%', ha='center', va='bottom', \n",
    "                        fontsize=11, fontweight='bold', \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('4_wealth_depression_gradient.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cca660-bd49-4fd7-b960-566401bf7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS - ANXIETY BY WEALTH QUINTILE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== PLOT 6: ANXIETY BY WEALTH QUINTILE ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Calculate the data\n",
    "contingency_table = pd.crosstab(analysis_df['wealth_quintile'], analysis_df['anxiety_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['wealth_quintile'], analysis_df['anxiety_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars\n",
    "categories = sorted(analysis_df['wealth_quintile'].unique())\n",
    "no_anxiety = [cross_tab.loc[cat, 0] for cat in categories]  # No anxiety percentages\n",
    "anxiety = [cross_tab.loc[cat, 1] for cat in categories]     # Anxiety percentages\n",
    "\n",
    "# Counts for labels\n",
    "no_anxiety_counts = [contingency_table.loc[cat, 0] for cat in categories]\n",
    "anxiety_counts = [contingency_table.loc[cat, 1] for cat in categories]\n",
    "total_counts = [contingency_table.loc[cat].sum() for cat in categories]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart with anxiety colors (Green & Purple)\n",
    "bars_no_anxiety = plt.bar(x_pos - bar_width/2, no_anxiety, bar_width, \n",
    "                          color='#90EE90', label='No Anxiety',  # Light Green\n",
    "                          edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_anxiety = plt.bar(x_pos + bar_width/2, anxiety, bar_width, \n",
    "                       color='#9370DB', label='Anxiety',        # Medium Purple\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_no_anxiety, no_anxiety, no_anxiety_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_anxiety, anxiety, anxiety_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Anxiety Prevalence by Wealth Quintile', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Wealth Quintile', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=8)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(no_anxiety + anxiety) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: ANXIETY BY WEALTH QUINTILE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, quintile in enumerate(categories):\n",
    "    summary_data.append({\n",
    "        'Wealth Quintile': quintile,\n",
    "        'Total N': total_counts[i],\n",
    "        'No Anxiety': f\"{no_anxiety[i]:.1f}% (n={no_anxiety_counts[i]})\",\n",
    "        'Anxiety': f\"{anxiety[i]:.1f}% (n={anxiety_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Anxiety prevalence varies significantly across wealth quintiles\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Anxiety prevalence is similar across wealth quintiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d8ee6-ee73-434a-a936-eb66805225b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOCIOECONOMIC GRADIENT PLOT\n",
    "# =============================================================================\n",
    "\n",
    "print(\" Creating Socioeconomic Gradient Plot...\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "if all(var in analysis_df.columns for var in ['wealth_quintile', 'anxiety_binary']):\n",
    "    wealth_order = ['Poorest', 'Poorer', 'Middle', 'Richer', 'Richest']\n",
    "    wealth_depression = analysis_df.groupby('wealth_quintile')['anxiety_binary'].mean() * 100\n",
    "    wealth_depression = wealth_depression.reindex(wealth_order)\n",
    "    \n",
    "    if not wealth_depression.empty:\n",
    "        # Remove any NaN values\n",
    "        valid_indices = [i for i, val in enumerate(wealth_depression.values) if np.isfinite(val)]\n",
    "        valid_values = [wealth_depression.values[i] for i in valid_indices]\n",
    "        valid_labels = [wealth_depression.index[i] for i in valid_indices]\n",
    "        \n",
    "        if valid_values:\n",
    "            plt.plot(range(len(valid_values)), valid_values, \n",
    "                    marker='o', linewidth=3, markersize=12, color='#e74c3c', \n",
    "                    markerfacecolor='white', markeredgewidth=3)\n",
    "            plt.title('Socioeconomic Gradient: Anxiety by Wealth Quintile\\nBDHS 2022', \n",
    "                     fontweight='bold', fontsize=16)\n",
    "            plt.xlabel('Wealth Quintile', fontsize=12)\n",
    "            plt.ylabel('Anxiety Prevalence (%)', fontsize=12)\n",
    "            plt.xticks(range(len(valid_values)), valid_labels, rotation=0)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ylim(0, max(valid_values) * 1.15)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, value in enumerate(valid_values):\n",
    "                plt.text(i, value + 1, f'{value:.1f}%', ha='center', va='bottom', \n",
    "                        fontsize=11, fontweight='bold', \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('4_wealth_depression_gradient.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a43424-691e-4e94-89dc-a82013eb1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS - PLOT: DEPRESSION BY EDUCATION LEVEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== PLOT: DEPRESSION BY EDUCATION LEVEL ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Calculate the data\n",
    "contingency_table = pd.crosstab(analysis_df['education_level'], analysis_df['depression_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['education_level'], analysis_df['depression_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars - ensure consistent order\n",
    "education_order = ['No education', 'Primary', 'Secondary', 'Higher']\n",
    "categories = [cat for cat in education_order if cat in cross_tab.index]\n",
    "\n",
    "no_depression = [cross_tab.loc[cat, 0] for cat in categories]  # No depression percentages\n",
    "depression = [cross_tab.loc[cat, 1] for cat in categories]     # Depression percentages\n",
    "\n",
    "# Counts for labels\n",
    "no_depression_counts = [contingency_table.loc[cat, 0] for cat in categories]\n",
    "depression_counts = [contingency_table.loc[cat, 1] for cat in categories]\n",
    "total_counts = [contingency_table.loc[cat].sum() for cat in categories]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart with depression colors\n",
    "bars_no_depression = plt.bar(x_pos - bar_width/2, no_depression, bar_width, \n",
    "                             color='lightblue', label='No Depression', \n",
    "                             edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_depression = plt.bar(x_pos + bar_width/2, depression, bar_width, \n",
    "                          color='coral', label='Depression',\n",
    "                          edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_no_depression, no_depression, no_depression_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_depression, depression, depression_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Depression Prevalence by Education Level', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Education Level', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(no_depression + depression) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: DEPRESSION BY EDUCATION LEVEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, education in enumerate(categories):\n",
    "    summary_data.append({\n",
    "        'Education Level': education,\n",
    "        'Total N': total_counts[i],\n",
    "        'No Depression': f\"{no_depression[i]:.1f}% (n={no_depression_counts[i]})\",\n",
    "        'Depression': f\"{depression[i]:.1f}% (n={depression_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Depression prevalence varies significantly across education levels\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Depression prevalence is similar across education levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e115c4-5d99-4ff2-8be5-fbc9d4cdaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS - PLOT: ANXIETY BY EDUCATION LEVEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== PLOT 8: ANXIETY BY EDUCATION LEVEL ===\")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Calculate the data\n",
    "contingency_table = pd.crosstab(analysis_df['education_level'], analysis_df['anxiety_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['education_level'], analysis_df['anxiety_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars - ensure consistent order\n",
    "education_order = ['No education', 'Primary', 'Secondary', 'Higher']\n",
    "categories = [cat for cat in education_order if cat in cross_tab.index]\n",
    "\n",
    "no_anxiety = [cross_tab.loc[cat, 0] for cat in categories]  # No anxiety percentages\n",
    "anxiety = [cross_tab.loc[cat, 1] for cat in categories]     # Anxiety percentages\n",
    "\n",
    "# Counts for labels\n",
    "no_anxiety_counts = [contingency_table.loc[cat, 0] for cat in categories]\n",
    "anxiety_counts = [contingency_table.loc[cat, 1] for cat in categories]\n",
    "total_counts = [contingency_table.loc[cat].sum() for cat in categories]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart with anxiety colors\n",
    "bars_no_anxiety = plt.bar(x_pos - bar_width/2, no_anxiety, bar_width, \n",
    "                          color='#90EE90', label='No Anxiety',  # Light Green\n",
    "                          edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_anxiety = plt.bar(x_pos + bar_width/2, anxiety, bar_width, \n",
    "                       color='#9370DB', label='Anxiety',        # Medium Purple\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_no_anxiety, no_anxiety, no_anxiety_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_anxiety, anxiety, anxiety_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=10, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Anxiety Prevalence by Education Level', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Education Level', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=11)\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(no_anxiety + anxiety) + 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: ANXIETY BY EDUCATION LEVEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, education in enumerate(categories):\n",
    "    summary_data.append({\n",
    "        'Education Level': education,\n",
    "        'Total N': total_counts[i],\n",
    "        'No Anxiety': f\"{no_anxiety[i]:.1f}% (n={no_anxiety_counts[i]})\",\n",
    "        'Anxiety': f\"{anxiety[i]:.1f}% (n={anxiety_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Anxiety prevalence varies significantly across education levels\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Anxiety prevalence is similar across education levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879569e-f2c5-468f-86f0-93e108a0965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS - PLOT: DEPRESSION BY REGION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== PLOT 9: DEPRESSION BY REGION ===\")\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Calculate the data\n",
    "contingency_table = pd.crosstab(analysis_df['region'], analysis_df['depression_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['region'], analysis_df['depression_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars - use natural order of regions\n",
    "categories = sorted(contingency_table.index)  # Sort regions alphabetically or use your preferred order\n",
    "\n",
    "no_depression = [cross_tab.loc[cat, 0] for cat in categories]  # No depression percentages\n",
    "depression = [cross_tab.loc[cat, 1] for cat in categories]     # Depression percentages\n",
    "\n",
    "# Counts for labels\n",
    "no_depression_counts = [contingency_table.loc[cat, 0] for cat in categories]\n",
    "depression_counts = [contingency_table.loc[cat, 1] for cat in categories]\n",
    "total_counts = [contingency_table.loc[cat].sum() for cat in categories]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart with depression colors\n",
    "bars_no_depression = plt.bar(x_pos - bar_width/2, no_depression, bar_width, \n",
    "                             color='lightblue', label='No Depression', \n",
    "                             edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_depression = plt.bar(x_pos + bar_width/2, depression, bar_width, \n",
    "                          color='coral', label='Depression',\n",
    "                          edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_no_depression, no_depression, no_depression_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=9, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_depression, depression, depression_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=9, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Depression Prevalence by Region', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Region', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=10, rotation=45, ha='right')\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(no_depression + depression) + 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: DEPRESSION BY REGION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, region in enumerate(categories):\n",
    "    summary_data.append({\n",
    "        'Region': region,\n",
    "        'Total N': total_counts[i],\n",
    "        'No Depression': f\"{no_depression[i]:.1f}% (n={no_depression_counts[i]})\",\n",
    "        'Depression': f\"{depression[i]:.1f}% (n={depression_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Depression prevalence varies significantly across regions\")\n",
    "    # Identify regions with highest and lowest prevalence\n",
    "    max_region = categories[depression.index(max(depression))]\n",
    "    min_region = categories[depression.index(min(depression))]\n",
    "    print(f\"• Highest depression prevalence: {max_region} ({max(depression):.1f}%)\")\n",
    "    print(f\"• Lowest depression prevalence: {min_region} ({min(depression):.1f}%)\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Depression prevalence is similar across regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27129f09-c9aa-4b1f-97b0-5459074e2a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DESCRIPTIVE ASSOCIATIONS - PLOT: ANXIETY BY REGION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=== PLOT 10: ANXIETY BY REGION ===\")\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Calculate the data\n",
    "contingency_table = pd.crosstab(analysis_df['region'], analysis_df['anxiety_binary'])\n",
    "cross_tab = pd.crosstab(analysis_df['region'], analysis_df['anxiety_binary'], normalize='index') * 100\n",
    "\n",
    "# Prepare data for clustered bars - use natural order of regions\n",
    "categories = sorted(contingency_table.index)\n",
    "\n",
    "no_anxiety = [cross_tab.loc[cat, 0] for cat in categories]  # No anxiety percentages\n",
    "anxiety = [cross_tab.loc[cat, 1] for cat in categories]     # Anxiety percentages\n",
    "\n",
    "# Counts for labels\n",
    "no_anxiety_counts = [contingency_table.loc[cat, 0] for cat in categories]\n",
    "anxiety_counts = [contingency_table.loc[cat, 1] for cat in categories]\n",
    "total_counts = [contingency_table.loc[cat].sum() for cat in categories]\n",
    "\n",
    "# Set the width and positions for the bars\n",
    "bar_width = 0.35\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "# Create clustered bar chart with anxiety colors\n",
    "bars_no_anxiety = plt.bar(x_pos - bar_width/2, no_anxiety, bar_width, \n",
    "                          color='#90EE90', label='No Anxiety',  # Light Green\n",
    "                          edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "bars_anxiety = plt.bar(x_pos + bar_width/2, anxiety, bar_width, \n",
    "                       color='#9370DB', label='Anxiety',        # Medium Purple\n",
    "                       edgecolor='black', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (bar, value, count) in enumerate(zip(bars_no_anxiety, no_anxiety, no_anxiety_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=9, color='black')\n",
    "\n",
    "for i, (bar, value, count) in enumerate(zip(bars_anxiety, anxiety, anxiety_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + 1, \n",
    "             f'{value:.1f}%\\n({count})', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=9, color='black')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Anxiety Prevalence by Region', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Percentage (%)', fontsize=12, fontweight='bold', labelpad=10)\n",
    "plt.xlabel('Region', fontsize=12, fontweight='bold', labelpad=10)\n",
    "\n",
    "# Set x-axis ticks and labels with sample sizes\n",
    "plt.xticks(x_pos, [f'{cat}\\n(n={total})' for cat, total in zip(categories, total_counts)], \n",
    "           fontsize=10, rotation=45, ha='right')\n",
    "\n",
    "# Professional styling\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Professional legend\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, \n",
    "           loc='upper right', fontsize=11)\n",
    "\n",
    "# Adjust y-limit to accommodate labels\n",
    "plt.ylim(0, max(no_anxiety + anxiety) + 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Professional summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BREAKDOWN: ANXIETY BY REGION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for i, region in enumerate(categories):\n",
    "    summary_data.append({\n",
    "        'Region': region,\n",
    "        'Total N': total_counts[i],\n",
    "        'No Anxiety': f\"{no_anxiety[i]:.1f}% (n={no_anxiety_counts[i]})\",\n",
    "        'Anxiety': f\"{anxiety[i]:.1f}% (n={anxiety_counts[i]})\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"Chi-square: χ²({dof}) = {chi2:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"• Statistically significant association (p < 0.05)\")\n",
    "    print(f\"• Anxiety prevalence varies significantly across regions\")\n",
    "    # Identify regions with highest and lowest prevalence\n",
    "    max_region = categories[anxiety.index(max(anxiety))]\n",
    "    min_region = categories[anxiety.index(min(anxiety))]\n",
    "    print(f\"• Highest anxiety prevalence: {max_region} ({max(anxiety):.1f}%)\")\n",
    "    print(f\"• Lowest anxiety prevalence: {min_region} ({min(anxiety):.1f}%)\")\n",
    "else:\n",
    "    print(f\"• No statistically significant association (p ≥ 0.05)\")\n",
    "    print(f\"• Anxiety prevalence is similar across regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105caac-6549-4773-b927-685fe49f3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BIVARIATE ANALYSIS - CORRELATION MATRIX (FULL MATRIX)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== CORRELATION MATRIX (FULL) ===\")\n",
    "\n",
    "# Select continuous variables that have been properly converted to numeric\n",
    "continuous_vars = ['HW70', 'HW71', 'HW72', 'mother_age', 'phq9_score', 'gad7_score', 'child_age_months', 'bmi']\n",
    "available_continuous = [var for var in continuous_vars if var in analysis_df.columns]\n",
    "\n",
    "print(f\"Available continuous variables: {available_continuous}\")\n",
    "\n",
    "if len(available_continuous) > 1:\n",
    "    # Drop rows with missing values in these columns for correlation\n",
    "    corr_data = analysis_df[available_continuous].dropna()\n",
    "    \n",
    "    print(f\"Sample size for correlation analysis: {len(corr_data)}\")\n",
    "    \n",
    "    correlation_matrix = corr_data.corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # REMOVED THE MASK to show full correlation matrix\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "               square=True, fmt='.2f', cbar_kws={'shrink': 0.8},\n",
    "               annot_kws={'size': 10})\n",
    "    plt.title('Correlation Matrix of Key Continuous Variables (Full Matrix)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFull correlation matrix:\")\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Print strong correlations (absolute value > 0.3)\n",
    "    print(\"\\nStrong correlations (|r| > 0.3):\")\n",
    "    strong_corrs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.3:\n",
    "                strong_corrs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_val))\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    strong_corrs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    for var1, var2, corr_val in strong_corrs:\n",
    "        print(f\"{var1} - {var2}: {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d1292-6f6a-4e92-8ade-5ed921eecca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHQ-9 DISTRIBUTION PLOT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📊 Creating PHQ-9 Distribution Plot...\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "if 'phq9_score' in analysis_df.columns:\n",
    "    phq_scores = analysis_df['phq9_score'].dropna()\n",
    "    \n",
    "    if len(phq_scores) > 0:\n",
    "        n, bins, patches = plt.hist(phq_scores, bins=20, alpha=0.7, color='#3498db', edgecolor='black')\n",
    "        plt.axvline(x=5, color='red', linestyle='--', linewidth=3, label='Clinical cutoff (≥5)')\n",
    "        plt.title('Distribution of PHQ-9 Depression Scores\\n', fontweight='bold', fontsize=16)\n",
    "        plt.xlabel('PHQ-9 Score', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_score = phq_scores.mean()\n",
    "        median_score = phq_scores.median()\n",
    "        plt.axvline(x=mean_score, color='orange', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {mean_score:.1f}')\n",
    "        plt.axvline(x=median_score, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'Median: {median_score:.1f}')\n",
    "        plt.legend(fontsize=11)\n",
    "        \n",
    "        # Add sample size and prevalence above cutoff\n",
    "        above_cutoff = (phq_scores >= 5).mean() * 100\n",
    "        plt.text(0.05, 0.95, f'Sample size: {len(phq_scores):,}', \n",
    "                transform=plt.gca().transAxes, fontsize=11,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        plt.text(0.05, 0.88, f'Above cutoff (≥5): {above_cutoff:.1f}%', \n",
    "                transform=plt.gca().transAxes, fontsize=11,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('7_phq9_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✅ Plot 7 saved as '7_phq9_distribution.png'\")\n",
    "else:\n",
    "    print(\"❌ 'phq9_score' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b626aa-7648-4299-898a-8db6b5d2e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GAD-7 DISTRIBUTION PLOT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating GAD-7 Distribution Plot...\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "if 'gad7_score' in analysis_df.columns:\n",
    "    gad7_scores = analysis_df['gad7_score'].dropna()\n",
    "    \n",
    "    if len(phq_scores) > 0:\n",
    "        n, bins, patches = plt.hist(gad7_scores, bins=20, alpha=0.7, color='#3498db', edgecolor='black')\n",
    "        plt.axvline(x=4, color='red', linestyle='--', linewidth=3, label='Clinical cutoff (≥4)')\n",
    "        plt.title('Distribution of GAD-7 Depression Scores\\n', fontweight='bold', fontsize=16)\n",
    "        plt.xlabel('GAD-7 Score', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_score = gad7_scores.mean()\n",
    "        median_score = gad7_scores.median()\n",
    "        plt.axvline(x=mean_score, color='orange', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {mean_score:.1f}')\n",
    "        plt.axvline(x=median_score, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'Median: {median_score:.1f}')\n",
    "        plt.legend(fontsize=11)\n",
    "        \n",
    "        # Add sample size and prevalence above cutoff\n",
    "        above_cutoff = (gad7_scores >= 4).mean() * 100\n",
    "        plt.text(0.05, 0.95, f'Sample size: {len(phq_scores):,}', \n",
    "                transform=plt.gca().transAxes, fontsize=11,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        plt.text(0.05, 0.88, f'Above cutoff (≥4): {above_cutoff:.1f}%', \n",
    "                transform=plt.gca().transAxes, fontsize=11,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('8_gad7_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✅ Plot 7 saved as '8_gad7_distribution.png'\")\n",
    "else:\n",
    "    print(\"❌ 'gad7_score' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7125d49-166f-4749-85ee-991d0caac70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTIVARIABLE REGRESSION ANALYSIS - STUNTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MULTIVARIABLE REGRESSION: STUNTING ===\")\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Prepare data for regression using analysis_df and correct variable names\n",
    "# Use the binary outcome variables created in Step 3\n",
    "model_vars = ['stunting_binary', 'depression_binary', 'anxiety_binary', \n",
    "              'mother_age', 'education_level', 'wealth_quintile', 'urban_rural',\n",
    "              'child_sex', 'child_age_months', 'birth_order','bmi']  # ADDED birth_order HERE\n",
    "\n",
    "model_data = analysis_df[model_vars].dropna()\n",
    "\n",
    "print(f\"Sample size for regression: {len(model_data)}\")\n",
    "\n",
    "if len(model_data) > 0:\n",
    "    # Use the cleaned variable names from analysis_df\n",
    "    formula = \"stunting_binary ~ depression_binary + anxiety_binary + birth_order + mother_age + C(education_level) + C(wealth_quintile) + C(urban_rural) + C(child_sex) + child_age_months+bmi\"\n",
    "    \n",
    "    try:\n",
    "        model = smf.logit(formula, data=model_data).fit(disp=False, maxiter=1000)\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        # Calculate odds ratios with confidence intervals\n",
    "        odds_ratios = pd.DataFrame({\n",
    "            'Variable': model.params.index,\n",
    "            'Coef': model.params.values,\n",
    "            'OR': np.exp(model.params.values),\n",
    "            'OR_2.5%': np.exp(model.conf_int()[0]),\n",
    "            'OR_97.5%': np.exp(model.conf_int()[1]),\n",
    "            'p_value': model.pvalues\n",
    "        })\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ODDS RATIOS FOR STUNTING (Adjusted Model)\")\n",
    "        print(\"=\"*60)\n",
    "        print(odds_ratios.round(3))\n",
    "        \n",
    "        # Print interpretation for key variables\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"KEY INTERPRETATIONS:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        depression_or = odds_ratios[odds_ratios['Variable'] == 'depression_binary']['OR'].values[0]\n",
    "        depression_p = odds_ratios[odds_ratios['Variable'] == 'depression_binary']['p_value'].values[0]\n",
    "        \n",
    "        anxiety_or = odds_ratios[odds_ratios['Variable'] == 'anxiety_binary']['OR'].values[0]\n",
    "        anxiety_p = odds_ratios[odds_ratios['Variable'] == 'anxiety_binary']['p_value'].values[0]\n",
    "        \n",
    "        # Also add interpretation for birth_order\n",
    "        birth_order_or = odds_ratios[odds_ratios['Variable'] == 'birth_order']['OR'].values[0]\n",
    "        birth_order_p = odds_ratios[odds_ratios['Variable'] == 'birth_order']['p_value'].values[0]\n",
    "        \n",
    "        print(f\"Maternal Depression: OR = {depression_or:.3f}, p = {depression_p:.3f}\")\n",
    "        if depression_p < 0.05:\n",
    "            if depression_or > 1:\n",
    "                print(f\"  → Children of depressed mothers have {((depression_or-1)*100):.1f}% higher odds of stunting\")\n",
    "            else:\n",
    "                print(f\"  → Children of depressed mothers have {((1-depression_or)*100):.1f}% lower odds of stunting\")\n",
    "        \n",
    "        print(f\"Maternal Anxiety: OR = {anxiety_or:.3f}, p = {anxiety_p:.3f}\")\n",
    "        if anxiety_p < 0.05:\n",
    "            if anxiety_or > 1:\n",
    "                print(f\"  → Children of anxious mothers have {((anxiety_or-1)*100):.1f}% higher odds of stunting\")\n",
    "            else:\n",
    "                print(f\"  → Children of anxious mothers have {((1-anxiety_or)*100):.1f}% lower odds of stunting\")\n",
    "                \n",
    "        print(f\"Birth Order: OR = {birth_order_or:.3f}, p = {birth_order_p:.3f}\")\n",
    "        if birth_order_p < 0.05:\n",
    "            if birth_order_or > 1:\n",
    "                print(f\"  → Each increase in birth order is associated with {((birth_order_or-1)*100):.1f}% higher odds of stunting\")\n",
    "            else:\n",
    "                print(f\"  → Each increase in birth order is associated with {((1-birth_order_or)*100):.1f}% lower odds of stunting\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in model fitting: {e}\")\n",
    "        print(\"Trying with fewer variables...\")\n",
    "        \n",
    "        # Simplified model as fallback\n",
    "        simple_formula = \"stunting_binary ~ depression_binary + anxiety_binary + birth_order + mother_age + child_age_months\"\n",
    "        simple_model = smf.logit(simple_formula, data=model_data).fit(disp=False, maxiter=1000)\n",
    "        print(simple_model.summary())\n",
    "else:\n",
    "    print(\"No complete cases for regression analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ae999-cfa3-4d8f-a855-d4feb1b28540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTIVARIABLE REGRESSION ANALYSIS - WASTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MULTIVARIABLE REGRESSION: WASTING ===\")\n",
    "\n",
    "# Prepare data for regression using analysis_df and correct variable names\n",
    "model_vars = ['wasting_binary', 'depression_binary', 'anxiety_binary', \n",
    "              'mother_age', 'education_level', 'wealth_quintile', 'urban_rural',\n",
    "              'child_sex', 'child_age_months','birth_order','bmi']\n",
    "\n",
    "model_data = analysis_df[model_vars].dropna()\n",
    "\n",
    "print(f\"Sample size for regression: {len(model_data)}\")\n",
    "\n",
    "if len(model_data) > 0:\n",
    "    # Use the cleaned variable names from analysis_df\n",
    "    formula = \"wasting_binary ~ depression_binary + anxiety_binary + birth_order + bmi+mother_age + C(education_level) + C(wealth_quintile) + C(urban_rural) + C(child_sex) + child_age_months\"\n",
    "    \n",
    "    try:\n",
    "        model = smf.logit(formula, data=model_data).fit(disp=False, maxiter=1000)\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        # Calculate odds ratios with confidence intervals\n",
    "        odds_ratios = pd.DataFrame({\n",
    "            'Variable': model.params.index,\n",
    "            'Coef': model.params.values,\n",
    "            'OR': np.exp(model.params.values),\n",
    "            'OR_2.5%': np.exp(model.conf_int()[0]),\n",
    "            'OR_97.5%': np.exp(model.conf_int()[1]),\n",
    "            'p_value': model.pvalues\n",
    "        })\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ODDS RATIOS FOR WASTING (Adjusted Model)\")\n",
    "        print(\"=\"*60)\n",
    "        print(odds_ratios.round(3))\n",
    "        \n",
    "        # Print interpretation for key variables\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"KEY INTERPRETATIONS:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        depression_or = odds_ratios[odds_ratios['Variable'] == 'depression_binary']['OR'].values[0]\n",
    "        depression_p = odds_ratios[odds_ratios['Variable'] == 'depression_binary']['p_value'].values[0]\n",
    "        \n",
    "        anxiety_or = odds_ratios[odds_ratios['Variable'] == 'anxiety_binary']['OR'].values[0]\n",
    "        anxiety_p = odds_ratios[odds_ratios['Variable'] == 'anxiety_binary']['p_value'].values[0]\n",
    "        \n",
    "        # Add interpretation for birth_order\n",
    "        birth_order_or = odds_ratios[odds_ratios['Variable'] == 'birth_order']['OR'].values[0]\n",
    "        birth_order_p = odds_ratios[odds_ratios['Variable'] == 'birth_order']['p_value'].values[0]\n",
    "        \n",
    "        print(f\"Maternal Depression: OR = {depression_or:.3f}, p = {depression_p:.3f}\")\n",
    "        if depression_p < 0.05:\n",
    "            if depression_or > 1:\n",
    "                print(f\"  → Children of depressed mothers have {((depression_or-1)*100):.1f}% higher odds of wasting\")\n",
    "            else:\n",
    "                print(f\"  → Children of depressed mothers have {((1-depression_or)*100):.1f}% lower odds of wasting\")\n",
    "        \n",
    "        print(f\"Maternal Anxiety: OR = {anxiety_or:.3f}, p = {anxiety_p:.3f}\")\n",
    "        if anxiety_p < 0.05:\n",
    "            if anxiety_or > 1:\n",
    "                print(f\"  → Children of anxious mothers have {((anxiety_or-1)*100):.1f}% higher odds of wasting\")\n",
    "            else:\n",
    "                print(f\"  → Children of anxious mothers have {((1-anxiety_or)*100):.1f}% lower odds of wasting\")\n",
    "                \n",
    "        print(f\"Birth Order: OR = {birth_order_or:.3f}, p = {birth_order_p:.3f}\")\n",
    "        if birth_order_p < 0.05:\n",
    "            if birth_order_or > 1:\n",
    "                print(f\"  → Each increase in birth order is associated with {((birth_order_or-1)*100):.1f}% higher odds of wasting\")\n",
    "            else:\n",
    "                print(f\"  → Each increase in birth order is associated with {((1-birth_order_or)*100):.1f}% lower odds of wasting\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in model fitting: {e}\")\n",
    "        print(\"Trying with fewer variables...\")\n",
    "        \n",
    "        # Simplified model as fallback\n",
    "        simple_formula = \"wasting_binary ~ depression_binary + anxiety_binary + birth_order + mother_age + child_age_months\"\n",
    "        simple_model = smf.logit(simple_formula, data=model_data).fit(disp=False, maxiter=1000)\n",
    "        print(simple_model.summary())\n",
    "else:\n",
    "    print(\"No complete cases for regression analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ea11c-9804-4a5d-8fa1-6a1c9add2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTIVARIABLE REGRESSION ANALYSIS - UNDERWEIGHT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MULTIVARIABLE REGRESSION: UNDERWEIGHT ===\")\n",
    "\n",
    "# Prepare data for regression using analysis_df and correct variable names\n",
    "model_vars = ['underweight_binary', 'depression_binary', 'anxiety_binary', \n",
    "              'mother_age', 'education_level', 'wealth_quintile', 'urban_rural',\n",
    "              'child_sex', 'child_age_months','birth_order','bmi']\n",
    "\n",
    "model_data = analysis_df[model_vars].dropna()\n",
    "\n",
    "print(f\"Sample size for regression: {len(model_data)}\")\n",
    "\n",
    "if len(model_data) > 0:\n",
    "    # Use the cleaned variable names from analysis_df\n",
    "    formula = \"underweight_binary ~ depression_binary + anxiety_binary + birth_order +bmi+ mother_age + C(education_level) + C(wealth_quintile) + C(urban_rural) + C(child_sex) + child_age_months\"\n",
    "    \n",
    "    try:\n",
    "        model = smf.logit(formula, data=model_data).fit(disp=False, maxiter=1000)\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        # Calculate odds ratios with confidence intervals\n",
    "        odds_ratios = pd.DataFrame({\n",
    "            'Variable': model.params.index,\n",
    "            'Coef': model.params.values,\n",
    "            'OR': np.exp(model.params.values),\n",
    "            'OR_2.5%': np.exp(model.conf_int()[0]),\n",
    "            'OR_97.5%': np.exp(model.conf_int()[1]),\n",
    "            'p_value': model.pvalues\n",
    "        })\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ODDS RATIOS FOR UNDERWEIGHT (Adjusted Model)\")\n",
    "        print(\"=\"*60)\n",
    "        print(odds_ratios.round(3))\n",
    "        \n",
    "        # Print interpretation for key variables\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"KEY INTERPRETATIONS:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        depression_or = odds_ratios[odds_ratios['Variable'] == 'depression_binary']['OR'].values[0]\n",
    "        depression_p = odds_ratios[odds_ratios['Variable'] == 'depression_binary']['p_value'].values[0]\n",
    "        \n",
    "        anxiety_or = odds_ratios[odds_ratios['Variable'] == 'anxiety_binary']['OR'].values[0]\n",
    "        anxiety_p = odds_ratios[odds_ratios['Variable'] == 'anxiety_binary']['p_value'].values[0]\n",
    "        \n",
    "        # Add interpretation for birth_order\n",
    "        birth_order_or = odds_ratios[odds_ratios['Variable'] == 'birth_order']['OR'].values[0]\n",
    "        birth_order_p = odds_ratios[odds_ratios['Variable'] == 'birth_order']['p_value'].values[0]\n",
    "        \n",
    "        print(f\"Maternal Depression: OR = {depression_or:.3f}, p = {depression_p:.3f}\")\n",
    "        if depression_p < 0.05:\n",
    "            if depression_or > 1:\n",
    "                print(f\"  → Children of depressed mothers have {((depression_or-1)*100):.1f}% higher odds of underweight\")\n",
    "            else:\n",
    "                print(f\"  → Children of depressed mothers have {((1-depression_or)*100):.1f}% lower odds of underweight\")\n",
    "        \n",
    "        print(f\"Maternal Anxiety: OR = {anxiety_or:.3f}, p = {anxiety_p:.3f}\")\n",
    "        if anxiety_p < 0.05:\n",
    "            if anxiety_or > 1:\n",
    "                print(f\"  → Children of anxious mothers have {((anxiety_or-1)*100):.1f}% higher odds of underweight\")\n",
    "            else:\n",
    "                print(f\"  → Children of anxious mothers have {((1-anxiety_or)*100):.1f}% lower odds of underweight\")\n",
    "                \n",
    "        print(f\"Birth Order: OR = {birth_order_or:.3f}, p = {birth_order_p:.3f}\")\n",
    "        if birth_order_p < 0.05:\n",
    "            if birth_order_or > 1:\n",
    "                print(f\"  → Each increase in birth order is associated with {((birth_order_or-1)*100):.1f}% higher odds of underweight\")\n",
    "            else:\n",
    "                print(f\"  → Each increase in birth order is associated with {((1-birth_order_or)*100):.1f}% lower odds of underweight\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in model fitting: {e}\")\n",
    "        print(\"Trying with fewer variables...\")\n",
    "        \n",
    "        # Simplified model as fallback\n",
    "        simple_formula = \"underweight_binary ~ depression_binary + anxiety_binary + birth_order + mother_age + child_age_months\"\n",
    "        simple_model = smf.logit(simple_formula, data=model_data).fit(disp=False, maxiter=1000)\n",
    "        print(simple_model.summary())\n",
    "else:\n",
    "    print(\"No complete cases for regression analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897267cd-f5bf-4f16-b048-f2681d392e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MACHINE LEARNING ANALYSIS - PREPARATION FOR ALL OUTCOMES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MACHINE LEARNING ANALYSIS PREPARATION FOR ALL OUTCOMES ===\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Prepare features using the cleaned variables from analysis_df\n",
    "features = ['depression_binary', 'anxiety_binary', 'mother_age', 'education_level', \n",
    "            'wealth_quintile', 'urban_rural', 'child_sex', 'child_age_months','birth_order','bmi','region']\n",
    "\n",
    "# Define all target outcomes using the correct binary variable names\n",
    "targets = {\n",
    "    'stunting_binary': 'Stunting',\n",
    "    'wasting_binary': 'Wasting', \n",
    "    'underweight_binary': 'Underweight',\n",
    "    'any_malnutrition': 'Any Malnutrition'\n",
    "}\n",
    "\n",
    "print(f\"Features: {features}\")\n",
    "print(f\"Targets: {list(targets.keys())}\")\n",
    "\n",
    "# Prepare data for ML using analysis_df\n",
    "X = analysis_df[features].copy()\n",
    "\n",
    "# Handle categorical variables by encoding them\n",
    "categorical_cols = ['education_level', 'wealth_quintile', 'urban_rural', 'child_sex','region']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Handle missing values - use median for numerical and mode for categorical\n",
    "numerical_cols = ['mother_age', 'child_age_months','birth_order','bmi']\n",
    "categorical_cols_encoded = ['education_level', 'wealth_quintile', 'urban_rural', 'child_sex','region']\n",
    "\n",
    "# Impute numerical features\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "X[numerical_cols] = imputer_num.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Impute categorical features (already encoded)\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_cols_encoded] = imputer_cat.fit_transform(X[categorical_cols_encoded])\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")\n",
    "print(f\"Feature names: {X.columns.tolist()}\")\n",
    "print(f\"Missing values after imputation: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Display feature information\n",
    "print(\"\\nFeature summary:\")\n",
    "for feature in features:\n",
    "    if feature in X.columns:\n",
    "        print(f\"{feature}: {X[feature].dtype}, unique values: {X[feature].nunique()}\")\n",
    "\n",
    "# Now prepare target variables\n",
    "print(\"\\nPreparing target variables...\")\n",
    "for target_var, target_name in targets.items():\n",
    "    if target_var in analysis_df.columns:\n",
    "        y = analysis_df[target_var].dropna()\n",
    "        common_index = X.index.intersection(y.index)\n",
    "        if len(common_index) > 0:\n",
    "            print(f\"{target_name}: {len(common_index)} samples available\")\n",
    "        else:\n",
    "            print(f\"{target_name}: No common samples with features\")\n",
    "    else:\n",
    "        print(f\"{target_name}: Target variable '{target_var}' not found in analysis_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f6808-713b-42f8-87ea-65cc4861054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MACHINE LEARNING - FOR STUNTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MACHINE LEARNING ANALYSIS: STUNTING ===\")\n",
    "\n",
    "# Import required scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use the correct target variable from analysis_df\n",
    "target = 'stunting_binary'\n",
    "y = analysis_df[target].copy()\n",
    "\n",
    "# Align X and y (use only rows that have both features and target)\n",
    "common_index = X.index.intersection(y.index)\n",
    "X_aligned = X.loc[common_index]\n",
    "y_aligned = y.loc[common_index]\n",
    "\n",
    "print(f\"Sample size for ML: {len(X_aligned)}\")\n",
    "print(f\"Stunting prevalence: {y_aligned.mean():.3f}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_aligned, y_aligned, test_size=0.3, random_state=42, stratify=y_aligned\n",
    ")\n",
    "\n",
    "# Scale features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "stunting_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    \n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        X_eval = X_test_scaled\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        X_eval = X_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    stunting_results[name] = {\n",
    "        'model': model,\n",
    "        'auc': auc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'feature_importance': None\n",
    "    }\n",
    "    \n",
    "    print(f\"AUC: {auc:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"Cross-validation AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Feature importance for tree-based models\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        stunting_results[name]['feature_importance'] = feature_importance\n",
    "        print(f\"Top 10 features: {feature_importance.head(10)['feature'].tolist()}\")\n",
    "\n",
    "# Store results\n",
    "ml_results = {'stunting': stunting_results}\n",
    "\n",
    "# Compare model performance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON FOR STUNTING PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    model: {metric: values[metric] for metric in ['auc', 'f1', 'precision', 'recall', 'cv_mean']}\n",
    "    for model, values in stunting_results.items()\n",
    "}).T\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8bfe7f-afa1-4654-869e-4f93c8427911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MACHINE LEARNING - FOR WASTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MACHINE LEARNING ANALYSIS: WASTING ===\")\n",
    "\n",
    "# Use the correct target variable from analysis_df\n",
    "target = 'wasting_binary'\n",
    "y = analysis_df[target].copy()\n",
    "\n",
    "# Align X and y (use only rows that have both features and target)\n",
    "common_index = X.index.intersection(y.index)\n",
    "X_aligned = X.loc[common_index]\n",
    "y_aligned = y.loc[common_index]\n",
    "\n",
    "print(f\"Sample size for ML: {len(X_aligned)}\")\n",
    "print(f\"Wasting prevalence: {y_aligned.mean():.3f}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_aligned, y_aligned, test_size=0.3, random_state=42, stratify=y_aligned\n",
    ")\n",
    "\n",
    "# Scale features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models (create fresh instances)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "wasting_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    \n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        X_eval = X_test_scaled\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        X_eval = X_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    wasting_results[name] = {\n",
    "        'model': model,\n",
    "        'auc': auc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'feature_importance': None\n",
    "    }\n",
    "    \n",
    "    print(f\"AUC: {auc:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"Cross-validation AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Feature importance for tree-based models\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        wasting_results[name]['feature_importance'] = feature_importance\n",
    "        print(f\"Top 10 features: {feature_importance.head(10)['feature'].tolist()}\")\n",
    "\n",
    "# Store results\n",
    "ml_results['wasting'] = wasting_results\n",
    "\n",
    "# Compare model performance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON FOR WASTING PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    model: {metric: values[metric] for metric in ['auc', 'f1', 'precision', 'recall', 'cv_mean']}\n",
    "    for model, values in wasting_results.items()\n",
    "}).T\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415fd20-7815-47c3-ba62-b070292bb58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MACHINE LEARNING - FOR UNDERWEIGHT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MACHINE LEARNING ANALYSIS: UNDERWEIGHT ===\")\n",
    "\n",
    "# Use the correct target variable from analysis_df\n",
    "target = 'underweight_binary'\n",
    "y = analysis_df[target].copy()\n",
    "\n",
    "# Align X and y (use only rows that have both features and target)\n",
    "common_index = X.index.intersection(y.index)\n",
    "X_aligned = X.loc[common_index]\n",
    "y_aligned = y.loc[common_index]\n",
    "\n",
    "print(f\"Sample size for ML: {len(X_aligned)}\")\n",
    "print(f\"Underweight prevalence: {y_aligned.mean():.3f}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_aligned, y_aligned, test_size=0.3, random_state=42, stratify=y_aligned\n",
    ")\n",
    "\n",
    "# Scale features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models (create fresh instances)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "underweight_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    \n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        X_eval = X_test_scaled\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        X_eval = X_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    underweight_results[name] = {\n",
    "        'model': model,\n",
    "        'auc': auc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'feature_importance': None\n",
    "    }\n",
    "    \n",
    "    print(f\"AUC: {auc:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"Cross-validation AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Feature importance for tree-based models\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        underweight_results[name]['feature_importance'] = feature_importance\n",
    "        print(f\"Top 10 features: {feature_importance.head(10)['feature'].tolist()}\")\n",
    "\n",
    "# Store results\n",
    "ml_results['underweight'] = underweight_results\n",
    "\n",
    "# Compare model performance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON FOR UNDERWEIGHT PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    model: {metric: values[metric] for metric in ['auc', 'f1', 'precision', 'recall', 'cv_mean']}\n",
    "    for model, values in underweight_results.items()\n",
    "}).T\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eeb8e1-5f33-46f4-aa23-7f6cb7bbe4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MACHINE LEARNING - COMPARISON ACROSS ALL OUTCOMES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MACHINE LEARNING PERFORMANCE COMPARISON ACROSS OUTCOMES ===\")\n",
    "\n",
    "# Create comparison across all outcomes\n",
    "comparison_data = []\n",
    "\n",
    "for outcome, results in ml_results.items():\n",
    "    for model_name, metrics in results.items():\n",
    "        comparison_data.append({\n",
    "            'Outcome': outcome,\n",
    "            'Model': model_name,\n",
    "            'AUC': metrics['auc'],\n",
    "            'F1_Score': metrics['f1'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'CV_AUC_Mean': metrics['cv_mean'],\n",
    "            'CV_AUC_Std': metrics['cv_std']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display best model for each outcome\n",
    "print(\"\\nBEST MODEL FOR EACH OUTCOME:\")\n",
    "for outcome in ml_results.keys():\n",
    "    outcome_df = comparison_df[comparison_df['Outcome'] == outcome]\n",
    "    best_model = outcome_df.loc[outcome_df['AUC'].idxmax()]\n",
    "    print(f\"{outcome.upper()}: {best_model['Model']} (AUC = {best_model['AUC']:.3f})\")\n",
    "\n",
    "# Plot comparison across outcomes\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# AUC comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "for model in models.keys():\n",
    "    model_data = comparison_df[comparison_df['Model'] == model]\n",
    "    plt.plot(model_data['Outcome'], model_data['AUC'], 'o-', label=model, linewidth=2, markersize=8)\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC Comparison Across Outcomes', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# F1-Score comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "for model in models.keys():\n",
    "    model_data = comparison_df[comparison_df['Model'] == model]\n",
    "    plt.plot(model_data['Outcome'], model_data['F1_Score'], 'o-', label=model, linewidth=2, markersize=8)\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('F1-Score Comparison Across Outcomes', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Precision comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "for model in models.keys():\n",
    "    model_data = comparison_df[comparison_df['Model'] == model]\n",
    "    plt.plot(model_data['Outcome'], model_data['Precision'], 'o-', label=model, linewidth=2, markersize=8)\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Comparison Across Outcomes', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Recall comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "for model in models.keys():\n",
    "    model_data = comparison_df[comparison_df['Model'] == model]\n",
    "    plt.plot(model_data['Outcome'], model_data['Recall'], 'o-', label=model, linewidth=2, markersize=8)\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall Comparison Across Outcomes', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetailed Performance Comparison:\")\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4305f7c-8345-478c-8c49-6c662e56ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE FOR BEST MODELS ACROSS OUTCOMES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== FEATURE IMPORTANCE FOR BEST MODELS ACROSS OUTCOMES ===\")\n",
    "\n",
    "# Find best model for each outcome and plot feature importance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, outcome in enumerate(ml_results.keys()):\n",
    "    # Get best model for this outcome\n",
    "    outcome_results = ml_results[outcome]\n",
    "    best_model_name = max(outcome_results, key=lambda x: outcome_results[x]['auc'])\n",
    "    best_model = outcome_results[best_model_name]['model']\n",
    "    \n",
    "    # Feature importance based on model type\n",
    "    if best_model_name == 'Logistic Regression':\n",
    "        # Logistic regression coefficients\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'coefficient': best_model.coef_[0],\n",
    "            'importance': np.abs(best_model.coef_[0])\n",
    "        }).sort_values('importance', ascending=True).tail(10)  # Top 10\n",
    "        \n",
    "        # Plot\n",
    "        colors = ['red' if x < 0 else 'blue' for x in feature_importance['coefficient']]\n",
    "        axes[idx].barh(feature_importance['feature'], feature_importance['coefficient'], color=colors)\n",
    "        axes[idx].set_xlabel('Coefficient Value')\n",
    "        \n",
    "    else:\n",
    "        # Tree-based feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True).tail(10)  # Top 10\n",
    "        \n",
    "        # Plot\n",
    "        axes[idx].barh(feature_importance['feature'], feature_importance['importance'])\n",
    "        axes[idx].set_xlabel('Feature Importance')\n",
    "    \n",
    "    axes[idx].set_title(f'{outcome.upper()}\\nBest Model: {best_model_name}', fontweight='bold')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance tables\n",
    "print(\"\\nTOP FEATURES FOR EACH OUTCOME:\")\n",
    "for outcome in ml_results.keys():\n",
    "    outcome_results = ml_results[outcome]\n",
    "    best_model_name = max(outcome_results, key=lambda x: outcome_results[x]['auc'])\n",
    "    best_model = outcome_results[best_model_name]['model']\n",
    "    \n",
    "    print(f\"\\n--- {outcome.upper()} (Best: {best_model_name}) ---\")\n",
    "    \n",
    "    if best_model_name == 'Logistic Regression':\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'coefficient': best_model.coef_[0],\n",
    "            'importance': np.abs(best_model.coef_[0])\n",
    "        }).sort_values('importance', ascending=False).head(10)\n",
    "        print(feature_importance.round(3))\n",
    "    else:\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(10)\n",
    "        print(feature_importance.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9d239-7d85-48d6-b899-1f7dd46a9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPROVED MACHINE LEARNING WITH CLASS IMBALANCE HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== IMPROVED ML WITH CLASS IMBALANCE HANDLING ===\")\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def improved_ml_analysis(target_name, target_var):\n",
    "    \"\"\"Run improved ML analysis with class imbalance handling\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"IMPROVED ANALYSIS: {target_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get target variable\n",
    "    y = analysis_df[target_var].copy()\n",
    "    \n",
    "    # Align X and y\n",
    "    common_index = X.index.intersection(y.index)\n",
    "    X_aligned = X.loc[common_index]\n",
    "    y_aligned = y.loc[common_index]\n",
    "    \n",
    "    print(f\"Sample size: {len(X_aligned)}\")\n",
    "    print(f\"Class distribution: {y_aligned.value_counts().to_dict()}\")\n",
    "    print(f\"Positive class prevalence: {y_aligned.mean():.3f}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aligned, y_aligned, test_size=0.3, random_state=42, stratify=y_aligned\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Compute class weights for imbalance handling\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    # Define improved models\n",
    "    models = {\n",
    "        'Logistic Regression (Balanced)': LogisticRegression(\n",
    "            random_state=42, max_iter=1000, class_weight='balanced'\n",
    "        ),\n",
    "        'Logistic Regression (Custom Weights)': LogisticRegression(\n",
    "            random_state=42, max_iter=1000, class_weight=class_weight_dict\n",
    "        ),\n",
    "        'Random Forest (Balanced)': RandomForestClassifier(\n",
    "            n_estimators=100, random_state=42, class_weight='balanced'\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=100, random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        \n",
    "        # Fit model\n",
    "        if 'Logistic' in name:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            X_eval = X_test_scaled\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            X_eval = X_test\n",
    "        \n",
    "        # Find optimal threshold using precision-recall curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "        optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        \n",
    "        # Predict with optimal threshold\n",
    "        y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics with optimal threshold\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        f1 = f1_score(y_test, y_pred_optimal)\n",
    "        precision_val = precision_score(y_test, y_pred_optimal)\n",
    "        recall_val = recall_score(y_test, y_pred_optimal)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'auc': auc,\n",
    "            'f1': f1,\n",
    "            'precision': precision_val,\n",
    "            'recall': recall_val,\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "        print(f\"AUC: {auc:.3f}\")\n",
    "        print(f\"F1-score: {f1:.3f}\")\n",
    "        print(f\"Precision: {precision_val:.3f}\")\n",
    "        print(f\"Recall: {recall_val:.3f}\")\n",
    "        print(f\"Cross-validation AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "        \n",
    "        # Detailed classification report\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred_optimal, target_names=['Negative', 'Positive']))\n",
    "        \n",
    "        # Feature importance for tree-based models\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            results[name]['feature_importance'] = feature_importance\n",
    "            print(f\"Top 10 features: {feature_importance.head(10)['feature'].tolist()}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run improved analysis for all targets\n",
    "improved_results = {}\n",
    "\n",
    "targets_to_improve = {\n",
    "    'stunting': 'stunting_binary',\n",
    "    'wasting': 'wasting_binary', \n",
    "    'underweight': 'underweight_binary'\n",
    "}\n",
    "\n",
    "for target_name, target_var in targets_to_improve.items():\n",
    "    improved_results[target_name] = improved_ml_analysis(target_name, target_var)\n",
    "\n",
    "# Compare all improved models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF IMPROVED MODELS ACROSS ALL TARGETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for target_name in targets_to_improve.keys():\n",
    "    print(f\"\\n{target_name.upper()}:\")\n",
    "    comparison_df = pd.DataFrame({\n",
    "        model: {metric: values[metric] for metric in ['auc', 'f1', 'precision', 'recall', 'optimal_threshold']}\n",
    "        for model, values in improved_results[target_name].items()\n",
    "    }).T\n",
    "    print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c8233-6f12-4588-a564-0e2e02f2b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 25. CHILD MORBIDITY ANALYSIS - DESCRIPTIVE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== CHILD MORBIDITY DESCRIPTIVE ANALYSIS ===\")\n",
    "\n",
    "# Use the binary morbidity variables created in Step 3\n",
    "morbidity_vars = {\n",
    "    'Diarrhea': 'diarrhea_binary',\n",
    "    'Fever': 'fever_binary', \n",
    "    'Cough': 'cough_binary',\n",
    "    'ARI (cough + breathing problems)': 'ari_binary'\n",
    "}\n",
    "\n",
    "available_morbidity = {desc: var for desc, var in morbidity_vars.items() if var in analysis_df.columns}\n",
    "\n",
    "if available_morbidity:\n",
    "    print(\"Available child morbidity variables:\")\n",
    "    \n",
    "    # Calculate prevalence using the binary variables\n",
    "    morbidity_prevalence = {}\n",
    "    morbidity_counts = {}\n",
    "    \n",
    "    for desc, var in available_morbidity.items():\n",
    "        prevalence = analysis_df[var].mean() * 100\n",
    "        count = analysis_df[var].sum()\n",
    "        total = analysis_df[var].count()\n",
    "        morbidity_prevalence[desc] = prevalence\n",
    "        morbidity_counts[desc] = (count, total)\n",
    "        print(f\"  {desc}: {prevalence:.2f}% ({count}/{total})\")\n",
    "    \n",
    "    # Plot morbidity prevalence\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "    bars = plt.bar(morbidity_prevalence.keys(), morbidity_prevalence.values(), \n",
    "                   color=colors[:len(morbidity_prevalence)], alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    plt.xlabel('Morbidity Type', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Prevalence (%)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Child Morbidity Prevalence \\n', fontsize=16, fontweight='bold')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add prevalence values on bars\n",
    "    for i, (morbidity, prevalence) in enumerate(morbidity_prevalence.items()):\n",
    "        count, total = morbidity_counts[morbidity]\n",
    "        plt.text(i, prevalence + 0.5, f\"{prevalence:.1f}%\\n({count}/{total})\", \n",
    "                ha='center', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional analysis: Morbidity by maternal mental health\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MORBIDITY BY MATERNAL MENTAL HEALTH STATUS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for morbidity_desc, morbidity_var in available_morbidity.items():\n",
    "        print(f\"\\n{morbidity_desc}:\")\n",
    "        # By depression status\n",
    "        no_depression = analysis_df[analysis_df['depression_binary'] == 0][morbidity_var].mean() * 100\n",
    "        with_depression = analysis_df[analysis_df['depression_binary'] == 1][morbidity_var].mean() * 100\n",
    "        \n",
    "        # By anxiety status\n",
    "        no_anxiety = analysis_df[analysis_df['anxiety_binary'] == 0][morbidity_var].mean() * 100\n",
    "        with_anxiety = analysis_df[analysis_df['anxiety_binary'] == 1][morbidity_var].mean() * 100\n",
    "        \n",
    "        print(f\"  No depression: {no_depression:.1f}%\")\n",
    "        print(f\"  With depression: {with_depression:.1f}%\")\n",
    "        print(f\"  No anxiety: {no_anxiety:.1f}%\")\n",
    "        print(f\"  With anxiety: {with_anxiety:.1f}%\")\n",
    "        \n",
    "        # Statistical test for depression association\n",
    "        from scipy.stats import chi2_contingency\n",
    "        contingency = pd.crosstab(analysis_df['depression_binary'], analysis_df[morbidity_var])\n",
    "        chi2, p_val, dof, expected = chi2_contingency(contingency)\n",
    "        print(f\"  Depression association: χ²={chi2:.3f}, p={p_val:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No child morbidity variables found in the dataset\")\n",
    "    \n",
    "    # Debug: Check what morbidity variables exist\n",
    "    print(\"\\nDebug - Available binary variables:\")\n",
    "    binary_vars = [col for col in analysis_df.columns if 'binary' in col]\n",
    "    for var in binary_vars:\n",
    "        print(f\"  {var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe105ab4-25b2-43a5-bded-047e9c0044d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 26. CHILD MORBIDITY ASSOCIATIONS WITH MATERNAL MENTAL HEALTH\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== ASSOCIATION BETWEEN MATERNAL MENTAL HEALTH AND CHILD MORBIDITY ===\")\n",
    "\n",
    "# Use the binary morbidity variables created in Step 3\n",
    "morbidity_vars = {\n",
    "    'Diarrhea': 'diarrhea_binary',\n",
    "    'Fever': 'fever_binary', \n",
    "    'Cough': 'cough_binary',\n",
    "    'ARI (cough + breathing problems)': 'ari_binary'\n",
    "}\n",
    "\n",
    "available_morbidity = {desc: var for desc, var in morbidity_vars.items() if var in analysis_df.columns}\n",
    "\n",
    "if available_morbidity:\n",
    "    morbidity_results = []\n",
    "    \n",
    "    for desc, var in available_morbidity.items():\n",
    "        print(f\"\\n--- {desc} ---\")\n",
    "        \n",
    "        # Association with depression - USE analysis_df\n",
    "        cross_tab_dep = pd.crosstab(analysis_df['depression_binary'], analysis_df[var])\n",
    "        if cross_tab_dep.shape == (2, 2):\n",
    "            chi2_dep, pval_dep, dof_dep, expected_dep = chi2_contingency(cross_tab_dep)\n",
    "            prev_dep = cross_tab_dep.iloc[1, 1] / cross_tab_dep.iloc[1, :].sum()\n",
    "            prev_no_dep = cross_tab_dep.iloc[0, 1] / cross_tab_dep.iloc[0, :].sum()\n",
    "            pr_dep = prev_dep / prev_no_dep if prev_no_dep > 0 else float('inf')\n",
    "            \n",
    "            print(f\"  Association with depression:\")\n",
    "            print(f\"    χ² = {chi2_dep:.3f}, p = {pval_dep:.4f}\")\n",
    "            print(f\"    Prevalence ratio = {pr_dep:.3f}\")\n",
    "            print(f\"    Depression: {prev_dep*100:.1f}% vs No depression: {prev_no_dep*100:.1f}%\")\n",
    "        \n",
    "        # Association with anxiety - USE analysis_df\n",
    "        cross_tab_anx = pd.crosstab(analysis_df['anxiety_binary'], analysis_df[var])\n",
    "        if cross_tab_anx.shape == (2, 2):\n",
    "            chi2_anx, pval_anx, dof_anx, expected_anx = chi2_contingency(cross_tab_anx)\n",
    "            prev_anx = cross_tab_anx.iloc[1, 1] / cross_tab_anx.iloc[1, :].sum()\n",
    "            prev_no_anx = cross_tab_anx.iloc[0, 1] / cross_tab_anx.iloc[0, :].sum()\n",
    "            pr_anx = prev_anx / prev_no_anx if prev_no_anx > 0 else float('inf')\n",
    "            \n",
    "            print(f\"  Association with anxiety:\")\n",
    "            print(f\"    χ² = {chi2_anx:.3f}, p = {pval_anx:.4f}\")\n",
    "            print(f\"    Prevalence ratio = {pr_anx:.3f}\")\n",
    "            print(f\"    Anxiety: {prev_anx*100:.1f}% vs No anxiety: {prev_no_anx*100:.1f}%\")\n",
    "        \n",
    "        # Calculate overall prevalence for this morbidity\n",
    "        overall_prevalence = analysis_df[var].mean() * 100\n",
    "        \n",
    "        morbidity_results.append({\n",
    "            'Morbidity': desc,\n",
    "            'Prevalence': overall_prevalence,\n",
    "            'Depression_PR': pr_dep,\n",
    "            'Depression_p': pval_dep,\n",
    "            'Anxiety_PR': pr_anx,\n",
    "            'Anxiety_p': pval_anx\n",
    "        })\n",
    "    \n",
    "    # Create summary table\n",
    "    morbidity_df = pd.DataFrame(morbidity_results)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY OF MORBIDITY ASSOCIATIONS WITH MATERNAL MENTAL HEALTH\")\n",
    "    print(\"=\"*80)\n",
    "    print(morbidity_df.round(3))\n",
    "    \n",
    "    # Plot associations\n",
    "    if not morbidity_df.empty:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Depression associations\n",
    "        bars1 = ax1.bar(morbidity_df['Morbidity'], morbidity_df['Depression_PR'], \n",
    "                color='lightcoral', alpha=0.7, edgecolor='darkred', linewidth=1.5)\n",
    "        ax1.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='No association (PR=1)')\n",
    "        ax1.set_xlabel('Morbidity Type', fontweight='bold')\n",
    "        ax1.set_ylabel('Prevalence Ratio', fontweight='bold')\n",
    "        ax1.set_title('Association with Maternal Depression\\n(Prevalence Ratio)', fontweight='bold', fontsize=14)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add p-value stars and prevalence values\n",
    "        for i, (bar, row) in enumerate(zip(bars1, morbidity_df.iterrows())):\n",
    "            _, row_data = row\n",
    "            height = bar.get_height()\n",
    "            if row_data['Depression_p'] < 0.001:\n",
    "                star = '***'\n",
    "            elif row_data['Depression_p'] < 0.01:\n",
    "                star = '**'\n",
    "            elif row_data['Depression_p'] < 0.05:\n",
    "                star = '*'\n",
    "            else:\n",
    "                star = 'ns'\n",
    "            ax1.text(i, height + 0.05, star, ha='center', fontweight='bold', fontsize=12)\n",
    "            ax1.text(i, height/2, f\"PR={row_data['Depression_PR']:.2f}\", \n",
    "                    ha='center', fontweight='bold', color='white')\n",
    "        \n",
    "        # Anxiety associations\n",
    "        bars2 = ax2.bar(morbidity_df['Morbidity'], morbidity_df['Anxiety_PR'], \n",
    "                color='lightblue', alpha=0.7, edgecolor='darkblue', linewidth=1.5)\n",
    "        ax2.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='No association (PR=1)')\n",
    "        ax2.set_xlabel('Morbidity Type', fontweight='bold')\n",
    "        ax2.set_ylabel('Prevalence Ratio', fontweight='bold')\n",
    "        ax2.set_title('Association with Maternal Anxiety\\n(Prevalence Ratio)', fontweight='bold', fontsize=14)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add p-value stars and prevalence values\n",
    "        for i, (bar, row) in enumerate(zip(bars2, morbidity_df.iterrows())):\n",
    "            _, row_data = row\n",
    "            height = bar.get_height()\n",
    "            if row_data['Anxiety_p'] < 0.001:\n",
    "                star = '***'\n",
    "            elif row_data['Anxiety_p'] < 0.01:\n",
    "                star = '**'\n",
    "            elif row_data['Anxiety_p'] < 0.05:\n",
    "                star = '*'\n",
    "            else:\n",
    "                star = 'ns'\n",
    "            ax2.text(i, height + 0.05, star, ha='center', fontweight='bold', fontsize=12)\n",
    "            ax2.text(i, height/2, f\"PR={row_data['Anxiety_PR']:.2f}\", \n",
    "                    ha='center', fontweight='bold', color='white')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print interpretation\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INTERPRETATION GUIDE:\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Prevalence Ratio (PR) Interpretation:\")\n",
    "        print(\"  PR > 1: Higher morbidity in children of mothers with mental health condition\")\n",
    "        print(\"  PR = 1: No association\")\n",
    "        print(\"  PR < 1: Lower morbidity in children of mothers with mental health condition\")\n",
    "        print(\"\\nStatistical Significance:\")\n",
    "        print(\"  *** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant\")\n",
    "\n",
    "else:\n",
    "    print(\"No child morbidity variables found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011e24e-e610-4f2b-ac69-0d34679a7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 27. MULTIVARIABLE REGRESSION FOR MORBIDITY OUTCOMES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MULTIVARIABLE REGRESSION FOR MORBIDITY OUTCOMES ===\")\n",
    "\n",
    "# Use the binary morbidity variables created in Step 3\n",
    "morbidity_vars = {\n",
    "    'Diarrhea': 'diarrhea_binary',\n",
    "    'Fever': 'fever_binary', \n",
    "    'Cough': 'cough_binary',\n",
    "    'ARI (cough + breathing problems)': 'ari_binary'\n",
    "}\n",
    "\n",
    "available_morbidity = {desc: var for desc, var in morbidity_vars.items() if var in analysis_df.columns}\n",
    "\n",
    "if available_morbidity:\n",
    "    for desc, var in available_morbidity.items():\n",
    "        print(f\"\\n--- {desc.upper()} ---\")\n",
    "        \n",
    "        # Prepare data for regression using analysis_df and cleaned variable names\n",
    "        model_vars = [var, 'depression_binary', 'anxiety_binary', \n",
    "                     'mother_age', 'education_level', 'wealth_quintile', 'urban_rural',\n",
    "                     'child_sex', 'child_age_months','birth_order']\n",
    "        \n",
    "        model_data = analysis_df[model_vars].dropna()\n",
    "        \n",
    "        print(f\"Sample size: {len(model_data)}\")\n",
    "        print(f\"Outcome prevalence: {model_data[var].mean()*100:.1f}%\")\n",
    "        \n",
    "        if len(model_data) > 0:\n",
    "            formula = f\"{var} ~ depression_binary + anxiety_binary+birth_order + mother_age + C(education_level) + C(wealth_quintile) + C(urban_rural) + C(child_sex) + child_age_months\"\n",
    "            \n",
    "            try:\n",
    "                model = smf.logit(formula, data=model_data).fit(disp=False, maxiter=1000)\n",
    "                \n",
    "                # Calculate odds ratios\n",
    "                odds_ratios = pd.DataFrame({\n",
    "                    'Variable': model.params.index,\n",
    "                    'Coef': model.params.values,\n",
    "                    'OR': np.exp(model.params.values),\n",
    "                    'OR_2.5%': np.exp(model.conf_int()[0]),\n",
    "                    'OR_97.5%': np.exp(model.conf_int()[1]),\n",
    "                    'p_value': model.pvalues\n",
    "                })\n",
    "                \n",
    "                print(f\"\\nAdjusted Odds Ratios for {desc}:\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                # Show key variables of interest\n",
    "                key_vars = ['depression_binary', 'anxiety_binary']\n",
    "                key_results = odds_ratios[odds_ratios['Variable'].isin(key_vars)]\n",
    "                \n",
    "                if not key_results.empty:\n",
    "                    for _, row in key_results.iterrows():\n",
    "                        significance = \"\"\n",
    "                        if row['p_value'] < 0.001:\n",
    "                            significance = \"***\"\n",
    "                        elif row['p_value'] < 0.01:\n",
    "                            significance = \"**\"\n",
    "                        elif row['p_value'] < 0.05:\n",
    "                            significance = \"*\"\n",
    "                        \n",
    "                        print(f\"{row['Variable']}: OR = {row['OR']:.3f} ({row['OR_2.5%']:.3f}-{row['OR_97.5%']:.3f}) {significance}\")\n",
    "                        \n",
    "                        # Interpretation\n",
    "                        if row['Variable'] == 'depression_binary':\n",
    "                            if row['OR'] > 1:\n",
    "                                print(f\"  → Children of depressed mothers have {((row['OR']-1)*100):.1f}% higher odds of {desc.lower()}\")\n",
    "                            elif row['OR'] < 1:\n",
    "                                print(f\"  → Children of depressed mothers have {((1-row['OR'])*100):.1f}% lower odds of {desc.lower()}\")\n",
    "                        \n",
    "                        elif row['Variable'] == 'anxiety_binary':\n",
    "                            if row['OR'] > 1:\n",
    "                                print(f\"  → Children of anxious mothers have {((row['OR']-1)*100):.1f}% higher odds of {desc.lower()}\")\n",
    "                            elif row['OR'] < 1:\n",
    "                                print(f\"  → Children of anxious mothers have {((1-row['OR'])*100):.1f}% lower odds of {desc.lower()}\")\n",
    "                \n",
    "                # Show model fit statistics\n",
    "                print(f\"\\nModel Fit:\")\n",
    "                print(f\"  AIC: {model.aic:.2f}\")\n",
    "                print(f\"  Log-Likelihood: {model.llf:.2f}\")\n",
    "                print(f\"  Pseudo R²: {model.prsquared:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Regression error for {desc}: {e}\")\n",
    "                print(\"Trying simplified model...\")\n",
    "                \n",
    "                # Try simplified model\n",
    "                try:\n",
    "                    simple_formula = f\"{var} ~ depression_binary + anxiety_binary + mother_age + child_age_months\"\n",
    "                    simple_model = smf.logit(simple_formula, data=model_data).fit(disp=False, maxiter=1000)\n",
    "                    \n",
    "                    simple_odds = pd.DataFrame({\n",
    "                        'Variable': simple_model.params.index,\n",
    "                        'OR': np.exp(simple_model.params.values),\n",
    "                        'p_value': simple_model.pvalues\n",
    "                    }).round(3)\n",
    "                    \n",
    "                    print(\"Simplified model results:\")\n",
    "                    key_simple = simple_odds[simple_odds['Variable'].isin(['depression_binary', 'anxiety_binary'])]\n",
    "                    print(key_simple)\n",
    "                    \n",
    "                except Exception as e2:\n",
    "                    print(f\"Simplified model also failed: {e2}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"No complete cases for regression analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"No child morbidity variables found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa2d56-30a8-4a34-8b00-ef1ccec81d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 28. MACHINE LEARNING FOR MORBIDITY OUTCOMES (FINAL FIX)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== MACHINE LEARNING FOR MORBIDITY OUTCOMES ===\")\n",
    "\n",
    "# Use the binary morbidity variables created in Step 3\n",
    "morbidity_vars = {\n",
    "    'Diarrhea': 'diarrhea_binary',\n",
    "    'Fever': 'fever_binary', \n",
    "    'Cough': 'cough_binary',\n",
    "    'ARI (cough + breathing problems)': 'ari_binary'\n",
    "}\n",
    "\n",
    "available_morbidity = {desc: var for desc, var in morbidity_vars.items() if var in analysis_df.columns}\n",
    "\n",
    "if available_morbidity:\n",
    "    morbidity_ml_results = {}\n",
    "    \n",
    "    for desc, var in available_morbidity.items():\n",
    "        print(f\"\\n=== MACHINE LEARNING: {desc.upper()} ===\")\n",
    "        \n",
    "        target = var\n",
    "        y = analysis_df[target].copy()\n",
    "        \n",
    "        # Align X and y (use only rows that have both features and target)\n",
    "        common_index = X.index.intersection(y.index)\n",
    "        X_aligned = X.loc[common_index]\n",
    "        y_aligned = y.loc[common_index]\n",
    "        \n",
    "        print(f\"Sample size for ML: {len(X_aligned)}\")\n",
    "        print(f\"{desc} prevalence: {y_aligned.mean():.3f}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_aligned, y_aligned, test_size=0.3, random_state=42, stratify=y_aligned\n",
    "        )\n",
    "        \n",
    "        # Check data types and handle encoding properly for ALL models\n",
    "        print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "        \n",
    "        # Convert ALL categorical variables to numerical using one-hot encoding for ALL models\n",
    "        categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "        \n",
    "        if len(categorical_cols) > 0:\n",
    "            print(f\"Categorical features to encode: {list(categorical_cols)}\")\n",
    "            \n",
    "            # One-hot encode categorical variables for ALL models\n",
    "            X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "            X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "            \n",
    "            # Align columns between train and test (in case some categories are missing in test set)\n",
    "            X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "            \n",
    "            print(f\"After encoding - Train shape: {X_train_encoded.shape}, Test shape: {X_test_encoded.shape}\")\n",
    "        else:\n",
    "            X_train_encoded = X_train.copy()\n",
    "            X_test_encoded = X_test.copy()\n",
    "        \n",
    "        # Scale features for Logistic Regression only (tree-based models don't need scaling)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "        X_test_scaled = scaler.transform(X_test_encoded)\n",
    "        \n",
    "        # Define models with proper class weighting for imbalanced data\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(\n",
    "                random_state=42, max_iter=1000, class_weight='balanced'\n",
    "            ),\n",
    "            'Random Forest': RandomForestClassifier(\n",
    "                n_estimators=100, random_state=42, class_weight='balanced'\n",
    "            ),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(\n",
    "                n_estimators=100, random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\n--- {name} ---\")\n",
    "            \n",
    "            try:\n",
    "                if name == 'Logistic Regression':\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                    X_eval = X_test_scaled\n",
    "                else:\n",
    "                    # Tree-based models use the encoded but not scaled data\n",
    "                    model.fit(X_train_encoded, y_train)\n",
    "                    y_pred = model.predict(X_test_encoded)\n",
    "                    y_pred_proba = model.predict_proba(X_test_encoded)[:, 1]\n",
    "                    X_eval = X_test_encoded\n",
    "                \n",
    "                # Calculate metrics\n",
    "                auc = roc_auc_score(y_test, y_pred_proba)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                \n",
    "                # Cross-validation\n",
    "                if name == 'Logistic Regression':\n",
    "                    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "                else:\n",
    "                    cv_scores = cross_val_score(model, X_train_encoded, y_train, cv=5, scoring='roc_auc')\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'auc': auc,\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'cv_mean': cv_scores.mean(),\n",
    "                    'cv_std': cv_scores.std(),\n",
    "                    'feature_importance': None\n",
    "                }\n",
    "                \n",
    "                print(f\"AUC: {auc:.3f}\")\n",
    "                print(f\"F1-score: {f1:.3f}\")\n",
    "                print(f\"Precision: {precision:.3f}\")\n",
    "                print(f\"Recall: {recall:.3f}\")\n",
    "                print(f\"Cross-validation AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "                \n",
    "                # Feature importance for tree-based models\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    if name == 'Logistic Regression':\n",
    "                        feature_names = X_train_encoded.columns\n",
    "                    else:\n",
    "                        feature_names = X_train_encoded.columns\n",
    "                    \n",
    "                    feature_importance = pd.DataFrame({\n",
    "                        'feature': feature_names,\n",
    "                        'importance': model.feature_importances_\n",
    "                    }).sort_values('importance', ascending=False)\n",
    "                    results[name]['feature_importance'] = feature_importance\n",
    "                    print(f\"Top 10 features: {feature_importance.head(10)['feature'].tolist()}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        morbidity_ml_results[desc] = results\n",
    "        \n",
    "        # Compare model performance for this morbidity\n",
    "        if results:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"MODEL COMPARISON FOR {desc.upper()} PREDICTION\")\n",
    "            print(\"=\"*60)\n",
    "            comparison_df = pd.DataFrame({\n",
    "                model: {metric: values[metric] for metric in ['auc', 'f1', 'precision', 'recall', 'cv_mean']}\n",
    "                for model, values in results.items()\n",
    "            }).T\n",
    "            print(comparison_df.round(3))\n",
    "    \n",
    "    # Add morbidity results to main ML results\n",
    "    ml_results.update(morbidity_ml_results)\n",
    "    \n",
    "    # Final summary across all morbidity outcomes\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY: MACHINE LEARNING PERFORMANCE ACROSS ALL MORBIDITY OUTCOMES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_data = []\n",
    "    for morbidity, models_dict in morbidity_ml_results.items():\n",
    "        for model_name, metrics in models_dict.items():\n",
    "            summary_data.append({\n",
    "                'Morbidity': morbidity,\n",
    "                'Model': model_name,\n",
    "                'AUC': metrics['auc'],\n",
    "                'F1-Score': metrics['f1'],\n",
    "                'Precision': metrics['precision'],\n",
    "                'Recall': metrics['recall'],\n",
    "                'CV_AUC': metrics['cv_mean']\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_pivot = summary_df.pivot(index='Morbidity', columns='Model', values=['AUC', 'F1-Score'])\n",
    "        print(summary_pivot.round(3))\n",
    "        \n",
    "        # Find best model for each morbidity\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BEST MODEL FOR EACH MORBIDITY OUTCOME (Based on AUC)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for morbidity in morbidity_ml_results.keys():\n",
    "            if morbidity_ml_results[morbidity]:\n",
    "                best_model = max(morbidity_ml_results[morbidity].items(), key=lambda x: x[1]['auc'])\n",
    "                print(f\"{morbidity}: {best_model[0]} (AUC = {best_model[1]['auc']:.3f})\")\n",
    "\n",
    "else:\n",
    "    print(\"No child morbidity variables found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710b0be-b234-4b58-b185-c43d7f1a64e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 29. PATHWAY ANALYSIS - MEDIATION THROUGH MORBIDITY (IMPROVED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== PATHWAY ANALYSIS: DOES MORBIDITY MEDIATE THE RELATIONSHIP? ===\")\n",
    "\n",
    "if available_morbidity:\n",
    "    print(\"Testing if child morbidity mediates the relationship between maternal mental health and child nutrition:\")\n",
    "    \n",
    "    mediation_summary = []\n",
    "    \n",
    "    for nutrition_outcome in ['stunting_binary', 'wasting_binary', 'underweight_binary']:\n",
    "        print(f\"\\n--- {nutrition_outcome.upper().replace('_BINARY', '')} ---\")\n",
    "        \n",
    "        # Prepare base control variables (including BMI as a regular variable)\n",
    "        control_vars = ['mother_age', 'education_level', 'wealth_quintile', 'urban_rural', 'child_sex', 'child_age_months', 'bmi']\n",
    "        \n",
    "        try:\n",
    "            # Model 1: Direct effect (without morbidity)\n",
    "            formula_direct = f\"{nutrition_outcome} ~ depression_binary + anxiety_binary + \" + \" + \".join([f\"C({var})\" if var in ['education_level', 'wealth_quintile', 'urban_rural', 'child_sex'] else var for var in control_vars])\n",
    "            \n",
    "            model_direct = smf.logit(formula_direct, data=analysis_df).fit(disp=False, maxiter=1000)\n",
    "            or_depression_direct = np.exp(model_direct.params['depression_binary'])\n",
    "            p_depression_direct = model_direct.pvalues['depression_binary']\n",
    "            or_anxiety_direct = np.exp(model_direct.params['anxiety_binary'])\n",
    "            p_anxiety_direct = model_direct.pvalues['anxiety_binary']\n",
    "            \n",
    "            print(f\"Direct effect (without morbidity):\")\n",
    "            print(f\"  Depression OR: {or_depression_direct:.3f} (p={p_depression_direct:.3f})\")\n",
    "            print(f\"  Anxiety OR: {or_anxiety_direct:.3f} (p={p_anxiety_direct:.3f})\")\n",
    "            \n",
    "            # Model 2: With morbidity adjustment\n",
    "            for morbidity_desc, morbidity_var in available_morbidity.items():\n",
    "                formula_mediated = f\"{nutrition_outcome} ~ depression_binary + anxiety_binary + {morbidity_var} + \" + \" + \".join([f\"C({var})\" if var in ['education_level', 'wealth_quintile', 'urban_rural', 'child_sex'] else var for var in control_vars])\n",
    "                \n",
    "                try:\n",
    "                    model_mediated = smf.logit(formula_mediated, data=analysis_df).fit(disp=False, maxiter=1000)\n",
    "                    or_depression_mediated = np.exp(model_mediated.params['depression_binary'])\n",
    "                    p_depression_mediated = model_mediated.pvalues['depression_binary']\n",
    "                    or_anxiety_mediated = np.exp(model_mediated.params['anxiety_binary'])\n",
    "                    p_anxiety_mediated = model_mediated.pvalues['anxiety_binary']\n",
    "                    or_morbidity = np.exp(model_mediated.params[morbidity_var])\n",
    "                    p_morbidity = model_mediated.pvalues[morbidity_var]\n",
    "                    \n",
    "                    # Calculate percentage change\n",
    "                    pct_change_dep = ((or_depression_mediated - or_depression_direct) / or_depression_direct) * 100\n",
    "                    pct_change_anx = ((or_anxiety_mediated - or_anxiety_direct) / or_anxiety_direct) * 100\n",
    "                    \n",
    "                    print(f\"\\nWith {morbidity_desc} adjustment:\")\n",
    "                    print(f\"  Depression OR: {or_depression_mediated:.3f} (p={p_depression_mediated:.3f}, {pct_change_dep:+.1f}% change)\")\n",
    "                    print(f\"  Anxiety OR: {or_anxiety_mediated:.3f} (p={p_anxiety_mediated:.3f}, {pct_change_anx:+.1f}% change)\")\n",
    "                    print(f\"  {morbidity_desc} OR: {or_morbidity:.3f} (p={p_morbidity:.3f})\")\n",
    "                    \n",
    "                    # Store for summary\n",
    "                    mediation_summary.append({\n",
    "                        'Nutrition': nutrition_outcome.replace('_binary', ''),\n",
    "                        'Morbidity': morbidity_desc,\n",
    "                        'Depression_Direct_OR': or_depression_direct,\n",
    "                        'Depression_Direct_p': p_depression_direct,\n",
    "                        'Depression_Mediated_OR': or_depression_mediated,\n",
    "                        'Depression_Mediated_p': p_depression_mediated,\n",
    "                        'Depression_Change_pct': pct_change_dep,\n",
    "                        'Anxiety_Direct_OR': or_anxiety_direct,\n",
    "                        'Anxiety_Direct_p': p_anxiety_direct,\n",
    "                        'Anxiety_Mediated_OR': or_anxiety_mediated,\n",
    "                        'Anxiety_Mediated_p': p_anxiety_mediated,\n",
    "                        'Anxiety_Change_pct': pct_change_anx,\n",
    "                        'Morbidity_OR': or_morbidity,\n",
    "                        'Morbidity_p': p_morbidity\n",
    "                    })\n",
    "                    \n",
    "                    # Test for mediation (Baron & Kenny criteria)\n",
    "                    mediation_detected = []\n",
    "                    \n",
    "                    # Criterion 1: Mental health affects nutrition (direct path)\n",
    "                    if p_depression_direct < 0.05 or p_anxiety_direct < 0.05:\n",
    "                        mediation_detected.append(\"Mental health → Nutrition path exists\")\n",
    "                    \n",
    "                    # Criterion 2: Mental health affects morbidity (path a)\n",
    "                    # We need to test this separately\n",
    "                    formula_path_a = f\"{morbidity_var} ~ depression_binary + anxiety_binary + \" + \" + \".join([f\"C({var})\" if var in ['education_level', 'wealth_quintile', 'urban_rural', 'child_sex'] else var for var in control_vars])\n",
    "                    model_path_a = smf.logit(formula_path_a, data=analysis_df).fit(disp=False, maxiter=1000)\n",
    "                    p_dep_path_a = model_path_a.pvalues.get('depression_binary', 1.0)\n",
    "                    p_anx_path_a = model_path_a.pvalues.get('anxiety_binary', 1.0)\n",
    "                    \n",
    "                    if p_dep_path_a < 0.05:\n",
    "                        mediation_detected.append(\"Depression → Morbidity path exists\")\n",
    "                    if p_anx_path_a < 0.05:\n",
    "                        mediation_detected.append(\"Anxiety → Morbidity path exists\")\n",
    "                    \n",
    "                    # Criterion 3: Morbidity affects nutrition (path b)\n",
    "                    if p_morbidity < 0.05:\n",
    "                        mediation_detected.append(\"Morbidity → Nutrition path exists\")\n",
    "                    \n",
    "                    # Criterion 4: Effect reduction in mediated model\n",
    "                    if abs(pct_change_dep) > 10 and p_depression_direct < 0.05:\n",
    "                        mediation_detected.append(f\"Depression effect reduced by {abs(pct_change_dep):.1f}%\")\n",
    "                    if abs(pct_change_anx) > 10 and p_anxiety_direct < 0.05:\n",
    "                        mediation_detected.append(f\"Anxiety effect reduced by {abs(pct_change_anx):.1f}%\")\n",
    "                    \n",
    "                    if mediation_detected:\n",
    "                        print(f\"  → Mediation indicators: {', '.join(mediation_detected)}\")\n",
    "                    else:\n",
    "                        print(f\"  → No strong mediation evidence\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in {morbidity_desc} model: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in direct effect model: {e}\")\n",
    "    \n",
    "    # Create comprehensive summary table\n",
    "    if mediation_summary:\n",
    "        mediation_df = pd.DataFrame(mediation_summary)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"COMPREHENSIVE MEDIATION ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Display key columns\n",
    "        summary_cols = ['Nutrition', 'Morbidity', 'Depression_Direct_OR', 'Depression_Direct_p', \n",
    "                       'Depression_Change_pct', 'Anxiety_Direct_OR', 'Anxiety_Direct_p', 'Anxiety_Change_pct']\n",
    "        \n",
    "        print(mediation_df[summary_cols].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663319c3-60e4-4fc5-bbfa-66ae9741bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 30. COMPREHENSIVE HEALTH OUTCOMES COMPARISON (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== COMPREHENSIVE HEALTH OUTCOMES COMPARISON ===\")\n",
    "\n",
    "# Combine all outcomes for comparison\n",
    "all_outcomes = {\n",
    "    'Nutritional Outcomes': ['stunting_binary', 'wasting_binary', 'underweight_binary'],\n",
    "    'Morbidity Outcomes': list(available_morbidity.keys()) if available_morbidity else []\n",
    "}\n",
    "\n",
    "# Calculate association strength for each outcome\n",
    "association_results = []\n",
    "\n",
    "# Nutritional outcomes - USE analysis_df NOT df\n",
    "for outcome in all_outcomes['Nutritional Outcomes']:\n",
    "    try:\n",
    "        # Depression association\n",
    "        formula = f\"{outcome} ~ depression_binary\"\n",
    "        model = smf.logit(formula, data=analysis_df).fit(disp=False, maxiter=2000)\n",
    "        or_dep = np.exp(model.params['depression_binary'])\n",
    "        p_dep = model.pvalues['depression_binary']\n",
    "        \n",
    "        # Anxiety association\n",
    "        formula = f\"{outcome} ~ anxiety_binary\"\n",
    "        model = smf.logit(formula, data=analysis_df).fit(disp=False, maxiter=2000)\n",
    "        or_anx = np.exp(model.params['anxiety_binary'])\n",
    "        p_anx = model.pvalues['anxiety_binary']\n",
    "        \n",
    "        association_results.append({\n",
    "            'Outcome': outcome.replace('_binary', ''),\n",
    "            'Type': 'Nutritional',\n",
    "            'Depression_OR': or_dep,\n",
    "            'Depression_p': p_dep,\n",
    "            'Anxiety_OR': or_anx,\n",
    "            'Anxiety_p': p_anx\n",
    "        })\n",
    "        print(f\"✓ Processed {outcome}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {outcome}: {e}\")\n",
    "\n",
    "# Morbidity outcomes - USE analysis_df NOT df\n",
    "for outcome_desc in all_outcomes['Morbidity Outcomes']:\n",
    "    try:\n",
    "        outcome_var = available_morbidity[outcome_desc]\n",
    "        \n",
    "        # Use proper statistical tests instead of simple prevalence ratios\n",
    "        # Depression association\n",
    "        formula = f\"{outcome_var} ~ depression_binary\"\n",
    "        model = smf.logit(formula, data=analysis_df).fit(disp=False, maxiter=2000)\n",
    "        or_dep = np.exp(model.params['depression_binary'])\n",
    "        p_dep = model.pvalues['depression_binary']\n",
    "        \n",
    "        # Anxiety association\n",
    "        formula = f\"{outcome_var} ~ anxiety_binary\"\n",
    "        model = smf.logit(formula, data=analysis_df).fit(disp=False, maxiter=2000)\n",
    "        or_anx = np.exp(model.params['anxiety_binary'])\n",
    "        p_anx = model.pvalues['anxiety_binary']\n",
    "        \n",
    "        association_results.append({\n",
    "            'Outcome': outcome_desc,\n",
    "            'Type': 'Morbidity',\n",
    "            'Depression_OR': or_dep,\n",
    "            'Depression_p': p_dep,\n",
    "            'Anxiety_OR': or_anx,\n",
    "            'Anxiety_p': p_anx\n",
    "        })\n",
    "        print(f\"✓ Processed {outcome_desc}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {outcome_desc}: {e}\")\n",
    "\n",
    "# Create results dataframe\n",
    "association_df = pd.DataFrame(association_results)\n",
    "\n",
    "# Plot comprehensive comparison\n",
    "if not association_df.empty:\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    colors = {'Nutritional': 'blue', 'Morbidity': 'red'}\n",
    "    markers = {'Nutritional': 'o', 'Morbidity': 's'}\n",
    "    sizes = {'Nutritional': 150, 'Morbidity': 120}\n",
    "    \n",
    "    for outcome_type in association_df['Type'].unique():\n",
    "        subset = association_df[association_df['Type'] == outcome_type]\n",
    "        \n",
    "        plt.scatter(subset['Depression_OR'], subset['Anxiety_OR'], \n",
    "                   label=outcome_type, \n",
    "                   s=sizes[outcome_type], \n",
    "                   alpha=0.7, \n",
    "                   c=colors[outcome_type],\n",
    "                   marker=markers[outcome_type],\n",
    "                   edgecolors='black',\n",
    "                   linewidth=1)\n",
    "        \n",
    "        # Add outcome labels with significance stars\n",
    "        for _, row in subset.iterrows():\n",
    "            # Determine significance markers\n",
    "            dep_sig = ''\n",
    "            if row['Depression_p'] < 0.001:\n",
    "                dep_sig = '***'\n",
    "            elif row['Depression_p'] < 0.01:\n",
    "                dep_sig = '**'\n",
    "            elif row['Depression_p'] < 0.05:\n",
    "                dep_sig = '*'\n",
    "                \n",
    "            anx_sig = ''\n",
    "            if row['Anxiety_p'] < 0.001:\n",
    "                anx_sig = '***'\n",
    "            elif row['Anxiety_p'] < 0.01:\n",
    "                anx_sig = '**'\n",
    "            elif row['Anxiety_p'] < 0.05:\n",
    "                anx_sig = '*'\n",
    "            \n",
    "            label = f\"{row['Outcome']}\\nDep{dep_sig} Anx{anx_sig}\"\n",
    "            \n",
    "            plt.annotate(label, \n",
    "                        (row['Depression_OR'], row['Anxiety_OR']),\n",
    "                        xytext=(8, 8), \n",
    "                        textcoords='offset points', \n",
    "                        fontweight='bold',\n",
    "                        fontsize=9,\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Add reference lines\n",
    "    plt.axhline(y=1, color='grey', linestyle='--', alpha=0.7, label='No association (OR=1)')\n",
    "    plt.axvline(x=1, color='grey', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Depression Odds Ratio (log scale)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Anxiety Odds Ratio (log scale)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Comprehensive Association Map:\\nMaternal Mental Health vs Child Health Outcomes', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Use log scale for better visualization of ORs\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE ASSOCIATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Format p-values with significance stars\n",
    "    def format_p_value(p_val):\n",
    "        if p_val < 0.001:\n",
    "            return f\"{p_val:.4f}***\"\n",
    "        elif p_val < 0.01:\n",
    "            return f\"{p_val:.4f}**\"\n",
    "        elif p_val < 0.05:\n",
    "            return f\"{p_val:.4f}*\"\n",
    "        else:\n",
    "            return f\"{p_val:.4f}\"\n",
    "    \n",
    "    summary_df = association_df.copy()\n",
    "    summary_df['Depression_p'] = summary_df['Depression_p'].apply(format_p_value)\n",
    "    summary_df['Anxiety_p'] = summary_df['Anxiety_p'].apply(format_p_value)\n",
    "    \n",
    "    print(summary_df.round(3))\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY INTERPRETATIONS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    significant_depression = association_df[association_df['Depression_p'] < 0.05]\n",
    "    significant_anxiety = association_df[association_df['Anxiety_p'] < 0.05]\n",
    "    \n",
    "    if len(significant_depression) > 0:\n",
    "        print(\"✓ Significant Depression Associations:\")\n",
    "        for _, row in significant_depression.iterrows():\n",
    "            direction = \"higher\" if row['Depression_OR'] > 1 else \"lower\"\n",
    "            print(f\"  • {row['Outcome']}: {row['Depression_OR']:.2f}x {direction} odds\")\n",
    "    else:\n",
    "        print(\"✗ No significant depression associations found\")\n",
    "    \n",
    "    if len(significant_anxiety) > 0:\n",
    "        print(\"✓ Significant Anxiety Associations:\")\n",
    "        for _, row in significant_anxiety.iterrows():\n",
    "            direction = \"higher\" if row['Anxiety_OR'] > 1 else \"lower\"\n",
    "            print(f\"  • {row['Outcome']}: {row['Anxiety_OR']:.2f}x {direction} odds\")\n",
    "    else:\n",
    "        print(\"✗ No significant anxiety associations found\")\n",
    "\n",
    "else:\n",
    "    print(\"No association results to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d7292-aee3-4530-b9ab-356c2e1a759d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
